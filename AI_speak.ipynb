{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMnnKxmI2Gmw0lN4K5zSMNU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tijanicica/ai-speak/blob/main/AI_speak.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCPW9Tnsr59o"
      },
      "outputs": [],
      "source": [
        "# AI Speak Takmiƒçenje"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalacija (samo jednom)\n",
        "!pip install -q torchcodec transformers torchaudio librosa wandb accelerate\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import math\n",
        "import time\n",
        "import gc\n",
        "import random\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "from typing import Optional, Dict, List, Tuple\n",
        "\n",
        "# Audio processing\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Model\n",
        "import librosa\n",
        "\n",
        "# Signal processing\n",
        "from scipy import signal\n",
        "from scipy.interpolate import interp1d\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "print(\"All packages imported successfully!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOzepQMntdNm",
        "outputId": "d65fe855-6d94-41fc-e976-64cca3d8c34d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/2.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m77.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hAll packages imported successfully!\n",
            "PyTorch version: 2.9.0+cu126\n",
            "CUDA available: True\n",
            "GPU: Tesla T4\n",
            "VRAM: 14.74 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Serbian phoneme set\n",
        "SERBIAN_PHONEMES = [\n",
        "    'a', 'b', 'c', 'ƒç', 'ƒá', 'd', 'd≈æ', 'ƒë', 'e', 'f', 'g', 'h', 'i', 'j',\n",
        "    'k', 'l', '«â', 'm', 'n', '«å', 'o', 'p', 'r', 's', '≈°', 't', 'u', 'v',\n",
        "    'z', '≈æ', 'sil', 'sp'\n",
        "]\n",
        "\n",
        "PHONEME_TO_IDX = {p: i for i, p in enumerate(SERBIAN_PHONEMES)}\n",
        "NUM_PHONEMES = len(SERBIAN_PHONEMES)\n",
        "\n",
        "\n",
        "class AdvancedAudioProcessor:\n",
        "    \"\"\"Wav2Vec2 XLS-R feature extraction\"\"\"\n",
        "\n",
        "    def __init__(self, model_name=\"facebook/wav2vec2-xls-r-300m\", device='cuda'):\n",
        "        self.device = device\n",
        "        self.feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n",
        "        self.model = Wav2Vec2Model.from_pretrained(model_name).to(device)\n",
        "        self.model.eval()\n",
        "\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def extract_features(self, waveform, sr=16000):\n",
        "        \"\"\"Extract multilingual speech representations\"\"\"\n",
        "        if sr != 16000:\n",
        "            resampler = T.Resample(sr, 16000)\n",
        "            waveform = resampler(waveform)\n",
        "\n",
        "        waveform = waveform / (waveform.abs().max() + 1e-8)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            inputs = self.feature_extractor(\n",
        "                waveform.squeeze().cpu().numpy(),\n",
        "                sampling_rate=16000,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "\n",
        "            outputs = self.model(\n",
        "                inputs.input_values.to(self.device),\n",
        "                output_hidden_states=True\n",
        "            )\n",
        "\n",
        "            # Weighted sum of last 4 layers\n",
        "            hidden_states = outputs.hidden_states[-4:]\n",
        "            features = torch.stack(hidden_states).mean(0).squeeze(0)\n",
        "\n",
        "        return features.cpu()\n",
        "\n",
        "\n",
        "class PhonemeProcessor:\n",
        "    \"\"\"Phoneme alignment processor\"\"\"\n",
        "\n",
        "    def __init__(self, fps=100):\n",
        "        self.fps = fps\n",
        "        self.phoneme_to_idx = PHONEME_TO_IDX\n",
        "        self.num_phonemes = NUM_PHONEMES\n",
        "\n",
        "    def parse_alignment_file(self, filepath):\n",
        "        \"\"\"Parse .txt file with format: start_time end_time phoneme\"\"\"\n",
        "        alignments = []\n",
        "        with open(filepath, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split()\n",
        "                if len(parts) >= 3:\n",
        "                    start, end, phoneme = float(parts[0]), float(parts[1]), parts[2]\n",
        "                    alignments.append((start, end, phoneme))\n",
        "        return alignments\n",
        "\n",
        "    def create_phoneme_indices(self, alignments, num_frames):\n",
        "        \"\"\"Create phoneme indices (not one-hot) for embedding\"\"\"\n",
        "        phoneme_indices = np.zeros(num_frames, dtype=np.int64)\n",
        "\n",
        "        for start_time, end_time, phoneme in alignments:\n",
        "            if phoneme not in self.phoneme_to_idx:\n",
        "                continue\n",
        "\n",
        "            start_frame = int(start_time * self.fps)\n",
        "            end_frame = int(end_time * self.fps)\n",
        "\n",
        "            start_frame = max(0, min(start_frame, num_frames - 1))\n",
        "            end_frame = max(0, min(end_frame, num_frames))\n",
        "\n",
        "            phoneme_idx = self.phoneme_to_idx[phoneme]\n",
        "            phoneme_indices[start_frame:end_frame] = phoneme_idx\n",
        "\n",
        "        return phoneme_indices\n",
        "\n",
        "\n",
        "class AggressiveDenoiser:\n",
        "    \"\"\"\n",
        "    KRITIƒåNO: MediaPipe labeli imaju MNOGO ≈°uma!\n",
        "    Agresivno filtriranje pre treninga (research preporuka)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.regions = {\n",
        "            'jaw': slice(24, 28),\n",
        "            'mouth': slice(28, 52),\n",
        "            'eyes': slice(5, 19),\n",
        "            'brows': slice(0, 5),\n",
        "            'other': slice(19, 24)\n",
        "        }\n",
        "\n",
        "    def savgol_filter(self, data, window, poly):\n",
        "        \"\"\"Savitzky-Golay with edge handling\"\"\"\n",
        "        if len(data) < window:\n",
        "            window = len(data) if len(data) % 2 == 1 else len(data) - 1\n",
        "        window = max(poly + 2, window)\n",
        "        if window % 2 == 0:\n",
        "            window += 1\n",
        "\n",
        "        try:\n",
        "            return signal.savgol_filter(data, window, poly, axis=0)\n",
        "        except:\n",
        "            return data\n",
        "\n",
        "    def denoise(self, blendshapes):\n",
        "        \"\"\"Smanjujemo agresivnost da bismo saƒçuvali dinamiku govora\"\"\"\n",
        "        denoised = blendshapes.copy()\n",
        "\n",
        "        # Brows: Smanjujemo sa 15 na 11\n",
        "        denoised[:, self.regions['brows']] = self.savgol_filter(\n",
        "            blendshapes[:, self.regions['brows']], 11, 3\n",
        "        )\n",
        "        # Eyes: Smanjujemo sa 13 na 9\n",
        "        denoised[:, self.regions['eyes']] = self.savgol_filter(\n",
        "            blendshapes[:, self.regions['eyes']], 9, 3\n",
        "        )\n",
        "        # Other: Smanjujemo sa 11 na 7\n",
        "        denoised[:, self.regions['other']] = self.savgol_filter(\n",
        "            blendshapes[:, self.regions['other']], 7, 3\n",
        "        )\n",
        "        # JAW: Smanjujemo sa 9 na 5 (KRITIƒåNO za brzinu vilice)\n",
        "        denoised[:, self.regions['jaw']] = self.savgol_filter(\n",
        "            blendshapes[:, self.regions['jaw']], 5, 3\n",
        "        )\n",
        "        # MOUTH: Smanjujemo sa 9 na 5 (KRITIƒåNO za o≈°trinu govora)\n",
        "        denoised[:, self.regions['mouth']] = self.savgol_filter(\n",
        "            blendshapes[:, self.regions['mouth']], 5, 3\n",
        "        )\n",
        "\n",
        "        return np.clip(denoised, 0, 1)\n",
        "\n",
        "    '''\n",
        "    def denoise(self, blendshapes):\n",
        "        \"\"\"Apply region-specific aggressive smoothing\"\"\"\n",
        "        denoised = blendshapes.copy()\n",
        "\n",
        "        # AGGRESSIVE smoothing (MediaPipe je jako ≈°uman)\n",
        "        denoised[:, self.regions['brows']] = self.savgol_filter(\n",
        "            blendshapes[:, self.regions['brows']], 15, 3\n",
        "        )\n",
        "        denoised[:, self.regions['eyes']] = self.savgol_filter(\n",
        "            blendshapes[:, self.regions['eyes']], 13, 3\n",
        "        )\n",
        "        denoised[:, self.regions['other']] = self.savgol_filter(\n",
        "            blendshapes[:, self.regions['other']], 11, 3\n",
        "        )\n",
        "        denoised[:, self.regions['jaw']] = self.savgol_filter(\n",
        "            blendshapes[:, self.regions['jaw']], 9, 3\n",
        "        )\n",
        "        denoised[:, self.regions['mouth']] = self.savgol_filter(\n",
        "            blendshapes[:, self.regions['mouth']], 9, 3\n",
        "        )\n",
        "\n",
        "        return np.clip(denoised, 0, 1)\n",
        "    '''\n",
        "\n",
        "class ChampionshipDataset(Dataset):\n",
        "    \"\"\"\n",
        "    SOTA Dataset sa svim research preporukama:\n",
        "    1. Aggressive denoising\n",
        "    2. Phoneme indices (for embedding)\n",
        "    3. Audio energy computation\n",
        "    4. Proper augmentation\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        base_path,\n",
        "        split='train',\n",
        "        fps=100,\n",
        "        augment=True,\n",
        "        device='cuda'\n",
        "    ):\n",
        "        self.base_path = base_path\n",
        "        self.fps = fps\n",
        "        self.augment = augment and split == 'train'\n",
        "        self.device = device\n",
        "\n",
        "        # Paths\n",
        "        self.blendshape_path = os.path.join(base_path, 'spk08_ser')\n",
        "        self.audio_path = os.path.join(base_path, 'ser/audio')\n",
        "        self.label_path = os.path.join(base_path, 'labels 08 srp')\n",
        "\n",
        "        # Get file list\n",
        "        self.file_ids = self._get_file_list()\n",
        "\n",
        "        # Train/val split (80/20)\n",
        "        split_idx = int(len(self.file_ids) * 0.8)\n",
        "        if split == 'train':\n",
        "            self.file_ids = self.file_ids[:split_idx]\n",
        "        else:\n",
        "            self.file_ids = self.file_ids[split_idx:]\n",
        "\n",
        "        # Initialize processors\n",
        "        self.audio_processor = AdvancedAudioProcessor(device=device)\n",
        "        self.phoneme_processor = PhonemeProcessor(fps=fps)\n",
        "        self.denoiser = AggressiveDenoiser()\n",
        "\n",
        "        print(f\"‚úÖ {split.upper()}: {len(self.file_ids)} samples\")\n",
        "\n",
        "    def _get_file_list(self):\n",
        "        \"\"\"Extract unique file IDs\"\"\"\n",
        "        npy_files = sorted([f for f in os.listdir(self.blendshape_path) if f.endswith('.npy')])\n",
        "        file_ids = [f.replace('.npy', '').replace('08_ser_spk08_', '') for f in npy_files]\n",
        "        return file_ids\n",
        "\n",
        "    def _load_audio(self, file_id):\n",
        "        \"\"\"Load audio\"\"\"\n",
        "        audio_file = os.path.join(self.audio_path, f\"spk08_{file_id}.wav\")\n",
        "        waveform, sr = torchaudio.load(audio_file)\n",
        "        return waveform, sr\n",
        "\n",
        "    def _augment_audio(self, waveform, sr):\n",
        "        \"\"\"Smart augmentation\"\"\"\n",
        "        if not self.augment or np.random.rand() > 0.5:\n",
        "            return waveform\n",
        "\n",
        "        # Time stretch (¬±10%)\n",
        "        if np.random.rand() > 0.5:\n",
        "            rate = np.random.uniform(0.9, 1.1)\n",
        "            waveform_np = waveform.squeeze().numpy()\n",
        "            waveform_np = librosa.effects.time_stretch(waveform_np, rate=rate)\n",
        "            waveform = torch.from_numpy(waveform_np).unsqueeze(0)\n",
        "\n",
        "        # Pitch shift (¬±2 semitones)\n",
        "        if np.random.rand() > 0.5:\n",
        "            n_steps = np.random.randint(-2, 3)\n",
        "            waveform_np = waveform.squeeze().numpy()\n",
        "            waveform_np = librosa.effects.pitch_shift(waveform_np, sr=sr, n_steps=n_steps)\n",
        "            waveform = torch.from_numpy(waveform_np).unsqueeze(0)\n",
        "\n",
        "        # Background noise (subtle)\n",
        "        if np.random.rand() > 0.7:\n",
        "            noise = torch.randn_like(waveform) * 0.005\n",
        "            waveform = waveform + noise\n",
        "\n",
        "        return waveform\n",
        "\n",
        "\n",
        "    def _compute_energy(self, waveform, num_frames):\n",
        "        \"\"\"\n",
        "        Raƒçuna RMS energiju iz audio signala i poravnava je sa brojem frejmova blendshape-ova.\n",
        "        \"\"\"\n",
        "        # Konverzija u numpy za librosa\n",
        "        y = waveform.squeeze().numpy()\n",
        "\n",
        "        # Parametri za 100 FPS (ako je sr=16000, hop_length 160 daje taƒçno 10ms po frejmu)\n",
        "        hop_length = 160\n",
        "        frame_length = 320\n",
        "\n",
        "        # Raƒçunanje RMS energije\n",
        "        rms = librosa.feature.rms(y=y, hop_length=hop_length, frame_length=frame_length)[0]\n",
        "\n",
        "        # Poravnanje du≈æine (librosa mo≈æe da vrati frejm vi≈°e ili manje zbog padding-a)\n",
        "        if len(rms) > num_frames:\n",
        "            rms = rms[:num_frames]\n",
        "        elif len(rms) < num_frames:\n",
        "            rms = np.pad(rms, (0, num_frames - len(rms)), mode='edge')\n",
        "\n",
        "        # Normalizacija na [0, 1] da bi te≈æine bile stabilne\n",
        "        rms_min = rms.min()\n",
        "        rms_max = rms.max()\n",
        "        if rms_max > rms_min:\n",
        "            rms = (rms - rms_min) / (rms_max - rms_min + 1e-8)\n",
        "\n",
        "        return rms.astype(np.float32)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_ids)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_id = self.file_ids[idx]\n",
        "\n",
        "        # 1. Load i Augmentacija audija\n",
        "        waveform, sr = self._load_audio(file_id)\n",
        "        waveform = self._augment_audio(waveform, sr)\n",
        "\n",
        "        # Obezbedi 16kHz verziju za ekstrakciju feature-a i energiju\n",
        "        if sr != 16000:\n",
        "            resampler = T.Resample(sr, 16000)\n",
        "            waveform_16k = resampler(waveform)\n",
        "        else:\n",
        "            waveform_16k = waveform\n",
        "\n",
        "        # 2. Extract Wav2Vec2 features (koristi waveform_16k)\n",
        "        audio_features = self.audio_processor.extract_features(waveform_16k, 16000)\n",
        "\n",
        "        # 3. Load blendshapes (Target)\n",
        "        bs_file = os.path.join(self.blendshape_path, f\"08_ser_spk08_{file_id}.npy\")\n",
        "        blendshapes = np.load(bs_file).astype(np.float32)\n",
        "        num_frames = blendshapes.shape[0]\n",
        "\n",
        "        # 4. CRITICAL: Aggressive denoising MediaPipe labela\n",
        "        blendshapes = self.denoiser.denoise(blendshapes)\n",
        "\n",
        "        # 5. Compute RMS Energy iz audija (za weighted loss)\n",
        "        # Sada prosleƒëujemo waveform i num_frames\n",
        "        energy = self._compute_energy(waveform_16k, num_frames)\n",
        "\n",
        "        # 6. Load phoneme alignments\n",
        "        label_file = os.path.join(self.label_path, f\"spk08_{file_id}.txt\")\n",
        "        alignments = self.phoneme_processor.parse_alignment_file(label_file)\n",
        "\n",
        "        # 7. Create phoneme indices (za embedding sloj)\n",
        "        phoneme_indices = self.phoneme_processor.create_phoneme_indices(alignments, num_frames)\n",
        "\n",
        "        # 8. Align audio features (50 FPS) na blendshape frames (100 FPS)\n",
        "        audio_len = audio_features.shape[0]\n",
        "        target_len = num_frames\n",
        "\n",
        "        if audio_len != target_len:\n",
        "            # Linearna interpolacija audio feature-a na 100 FPS\n",
        "            old_indices = np.linspace(0, audio_len - 1, audio_len)\n",
        "            new_indices = np.linspace(0, audio_len - 1, target_len)\n",
        "            interpolator = interp1d(old_indices, audio_features.numpy(), axis=0, kind='linear')\n",
        "            audio_features = torch.from_numpy(interpolator(new_indices)).float()\n",
        "\n",
        "        return {\n",
        "            'audio_features': audio_features,  # (T, 1024)\n",
        "            'phoneme_indices': torch.from_numpy(phoneme_indices).long(),  # (T,)\n",
        "            'blendshapes': torch.from_numpy(blendshapes),  # (T, 52)\n",
        "            'energy': torch.from_numpy(energy),  # (T,)\n",
        "            'file_id': file_id\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "def create_dataloaders(base_path, batch_size=8, num_workers=0, device='cuda'):\n",
        "    \"\"\"Create train/val dataloaders with proper collation\"\"\"\n",
        "\n",
        "    train_dataset = ChampionshipDataset(\n",
        "        base_path, split='train', augment=True, device=device\n",
        "    )\n",
        "    val_dataset = ChampionshipDataset(\n",
        "        base_path, split='val', augment=False, device=device\n",
        "    )\n",
        "\n",
        "    def collate_fn(batch):\n",
        "        \"\"\"Handle variable-length sequences\"\"\"\n",
        "        max_len = max([item['audio_features'].shape[0] for item in batch])\n",
        "\n",
        "        audio_padded = []\n",
        "        phoneme_padded = []\n",
        "        bs_padded = []\n",
        "        energy_padded = []\n",
        "        masks = []\n",
        "\n",
        "        for item in batch:\n",
        "            seq_len = item['audio_features'].shape[0]\n",
        "            pad_len = max_len - seq_len\n",
        "\n",
        "            # Pad sequences\n",
        "            audio_padded.append(\n",
        "                torch.cat([item['audio_features'],\n",
        "                          torch.zeros(pad_len, 1024)], dim=0)\n",
        "            )\n",
        "            phoneme_padded.append(\n",
        "                torch.cat([item['phoneme_indices'],\n",
        "                          torch.zeros(pad_len, dtype=torch.long)], dim=0)\n",
        "            )\n",
        "            bs_padded.append(\n",
        "                torch.cat([item['blendshapes'],\n",
        "                          torch.zeros(pad_len, 52)], dim=0)\n",
        "            )\n",
        "            energy_padded.append(\n",
        "                torch.cat([item['energy'],\n",
        "                          torch.zeros(pad_len)], dim=0)\n",
        "            )\n",
        "\n",
        "            # Create mask\n",
        "            mask = torch.zeros(max_len, dtype=torch.bool)\n",
        "            mask[:seq_len] = True\n",
        "            masks.append(mask)\n",
        "\n",
        "        return {\n",
        "            'audio_features': torch.stack(audio_padded),\n",
        "            'phoneme_indices': torch.stack(phoneme_padded),\n",
        "            'blendshapes': torch.stack(bs_padded),\n",
        "            'energy': torch.stack(energy_padded),\n",
        "            'mask': torch.stack(masks)\n",
        "        }\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset, batch_size=batch_size, shuffle=True,\n",
        "        collate_fn=collate_fn, num_workers=num_workers, pin_memory=True\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset, batch_size=batch_size, shuffle=False,\n",
        "        collate_fn=collate_fn, num_workers=num_workers, pin_memory=True\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader\n",
        "\n",
        "\n",
        "print(\"Championship Dataset loaded!\")\n",
        "print(\"Key features:\")\n",
        "print(\"   - Aggressive MediaPipe denoising\")\n",
        "print(\"   - Phoneme embedding (learned space)\")\n",
        "print(\"   - Energy-aware weighting\")\n",
        "print(\"   - Smart augmentation\")\n"
      ],
      "metadata": {
        "id": "b58zjuSquG0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"\n",
        "üèÜ CHAMPIONSHIP LIP-SYNC SYSTEM - ƒÜELIJA 3\n",
        "SOTA Model Architecture combining:\n",
        "- VQ-VAE (CodeTalker) - Discrete motion codes\n",
        "- Autoregressive GRU (FaceFormer) - Temporal consistency\n",
        "- Phoneme Embedding - Learned phoneme space\n",
        "- Multi-scale Discriminator - Fine + coarse motion\n",
        "\"\"\"\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. VECTOR QUANTIZATION (CodeTalker)\n",
        "# ==============================================================================\n",
        "\n",
        "class VectorQuantizer(nn.Module):\n",
        "    \"\"\"\n",
        "    Discrete motion codebook - prevents mode collapse\n",
        "    Research: CodeTalker (CVPR 2023)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_embeddings=512, embedding_dim=256, commitment_cost=0.25):\n",
        "        super().__init__()\n",
        "        self.num_embeddings = num_embeddings\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.commitment_cost = commitment_cost\n",
        "\n",
        "        self.embeddings = nn.Embedding(num_embeddings, embedding_dim)\n",
        "        self.embeddings.weight.data.uniform_(-1/num_embeddings, 1/num_embeddings)\n",
        "\n",
        "    def forward(self, z):\n",
        "        \"\"\"Quantize continuous features to discrete codes\"\"\"\n",
        "        B, T, D = z.shape\n",
        "        z_flattened = z.reshape(-1, D)\n",
        "\n",
        "        # Calculate distances to codebook\n",
        "        distances = (\n",
        "            torch.sum(z_flattened**2, dim=1, keepdim=True)\n",
        "            + torch.sum(self.embeddings.weight**2, dim=1)\n",
        "            - 2 * torch.matmul(z_flattened, self.embeddings.weight.t())\n",
        "        )\n",
        "\n",
        "        # Get nearest codebook entry\n",
        "        encoding_indices = torch.argmin(distances, dim=1)\n",
        "        encodings = F.one_hot(encoding_indices, self.num_embeddings).float()\n",
        "\n",
        "        # Quantize\n",
        "        quantized = torch.matmul(encodings, self.embeddings.weight)\n",
        "        quantized = quantized.view(B, T, D)\n",
        "\n",
        "        # Loss\n",
        "        e_latent_loss = F.mse_loss(quantized.detach(), z)\n",
        "        q_latent_loss = F.mse_loss(quantized, z.detach())\n",
        "        codebook_loss = q_latent_loss + self.commitment_cost * e_latent_loss\n",
        "\n",
        "        # Straight-through estimator\n",
        "        quantized = z + (quantized - z).detach()\n",
        "\n",
        "        return quantized, codebook_loss, encoding_indices.view(B, T)\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. POSITIONAL ENCODING\n",
        "# ==============================================================================\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"Sinusoidal positional encoding\"\"\"\n",
        "\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. CHAMPIONSHIP MODEL (VQ-VAE + Autoregressive GRU)\n",
        "# ==============================================================================\n",
        "\n",
        "class ChampionshipLipSyncModel(nn.Module):\n",
        "    \"\"\"\n",
        "    SOTA Lip-Sync Model combining best research practices:\n",
        "\n",
        "    1. Phoneme Embedding (learned, not one-hot)\n",
        "    2. Transformer Encoder (audio + phoneme fusion)\n",
        "    3. VQ-VAE (discrete motion priors)\n",
        "    4. Autoregressive GRU (temporal consistency)\n",
        "    5. Region-specific decoders (jaw, mouth, eyes, etc.)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        audio_dim=1024,\n",
        "        phoneme_embedding_dim=64,\n",
        "        d_model=512,\n",
        "        num_encoder_layers=4,\n",
        "        num_heads=8,\n",
        "        dim_feedforward=2048,\n",
        "        num_blendshapes=52,\n",
        "        codebook_size=512,\n",
        "        gru_hidden=128,\n",
        "        dropout=0.1\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.gru_hidden = gru_hidden\n",
        "\n",
        "        # ============ PHONEME EMBEDDING (learned space) ============\n",
        "        self.phoneme_embedding = nn.Embedding(NUM_PHONEMES, phoneme_embedding_dim)\n",
        "\n",
        "        # ============ INPUT PROJECTION ============\n",
        "        self.input_projection = nn.Linear(audio_dim + phoneme_embedding_dim, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model)\n",
        "\n",
        "        # ============ TRANSFORMER ENCODER ============\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            activation='gelu',\n",
        "            batch_first=True,\n",
        "            norm_first=True\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_encoder_layers)\n",
        "\n",
        "        # ============ VECTOR QUANTIZATION ============\n",
        "        self.pre_quant = nn.Linear(d_model, 256)\n",
        "        self.vq = VectorQuantizer(num_embeddings=codebook_size, embedding_dim=256)\n",
        "        self.post_quant = nn.Linear(256, d_model)\n",
        "\n",
        "        # ============ AUTOREGRESSIVE GRU (FaceFormer style) ============\n",
        "        # KRITIƒåNO: Ovo dodaje \"memory\" - model zna ≈°ta je rekao u prethodnom frejmu\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=d_model,\n",
        "            hidden_size=gru_hidden,\n",
        "            num_layers=1,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_encoder_layers > 1 else 0\n",
        "        )\n",
        "\n",
        "        # ============ OUTPUT HEADS (region-specific) ============\n",
        "        self.jaw_head = self._make_head(gru_hidden, 4)\n",
        "        self.mouth_head = self._make_head(gru_hidden, 24)\n",
        "        self.eye_head = self._make_head(gru_hidden, 14)\n",
        "        self.brow_head = self._make_head(gru_hidden, 5)\n",
        "        self.other_head = self._make_head(gru_hidden, 5)\n",
        "\n",
        "    def _make_head(self, input_dim, num_outputs):\n",
        "        head = nn.Sequential(\n",
        "            nn.Linear(input_dim, input_dim // 2),\n",
        "            nn.LayerNorm(input_dim // 2),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(input_dim // 2, num_outputs),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        # FORCE INITIALIZATION TO ZERO\n",
        "        # Postavljamo bias poslednjeg sloja na -3.0 tako da Sigmoid krene od ~0.05 (skoro zatvoreno)\n",
        "        nn.init.constant_(head[-2].bias, -3.0)\n",
        "        return head\n",
        "\n",
        "\n",
        "    def forward(self, audio_features, phoneme_indices, mask=None, hidden_state=None):\n",
        "        \"\"\"\n",
        "        A≈ΩURIRANO: VQ-Bypass za maksimalnu stabilnost na malom datasetu\n",
        "        \"\"\"\n",
        "        B, T, _ = audio_features.shape\n",
        "\n",
        "        # ============ PHONEME EMBEDDING ============\n",
        "        phoneme_emb = self.phoneme_embedding(phoneme_indices)\n",
        "\n",
        "        # ============ COMBINE AUDIO + PHONEME ============\n",
        "        x = torch.cat([audio_features, phoneme_emb], dim=-1)\n",
        "        x = self.input_projection(x)\n",
        "        x = self.pos_encoder(x)\n",
        "\n",
        "        # ============ ENCODE (Transformer) ============\n",
        "        if mask is not None:\n",
        "            src_key_padding_mask = ~mask\n",
        "        else:\n",
        "            src_key_padding_mask = None\n",
        "\n",
        "        encoded = self.encoder(x, src_key_padding_mask=src_key_padding_mask)\n",
        "\n",
        "        # ============ BYPASS VQ LAYER (IZMENA OVDE) ============\n",
        "        # Umesto VQ-a, ≈°aljemo direktno iz Transformera u GRU\n",
        "        # To je stabilnije za mali broj snimaka (80 reƒçenica)\n",
        "        quantized = encoded\n",
        "        vq_loss = torch.tensor(0.0).to(x.device) # Loss postavljamo na 0 da ne kvari trening\n",
        "\n",
        "        # ============ AUTOREGRESSIVE GRU ============\n",
        "        if hidden_state is None:\n",
        "            gru_out, hidden_state = self.gru(quantized)\n",
        "        else:\n",
        "            gru_out, hidden_state = self.gru(quantized, hidden_state)\n",
        "\n",
        "        # ============ BLENDSHAPE PREDICTION ============\n",
        "        jaw = self.jaw_head(gru_out)\n",
        "        mouth = self.mouth_head(gru_out)\n",
        "        eye = self.eye_head(gru_out)\n",
        "        brow = self.brow_head(gru_out)\n",
        "        other = self.other_head(gru_out)\n",
        "\n",
        "        blendshapes = torch.cat([brow, eye, other, jaw, mouth], dim=-1)\n",
        "\n",
        "        return blendshapes, vq_loss, hidden_state\n",
        "\n",
        "    '''\n",
        "    def forward(self, audio_features, phoneme_indices, mask=None, hidden_state=None):\n",
        "        \"\"\"\n",
        "        audio_features: (B, T, 1024)\n",
        "        phoneme_indices: (B, T) - integer indices\n",
        "        mask: (B, T) - boolean mask\n",
        "        hidden_state: Optional GRU hidden state for autoregression\n",
        "        \"\"\"\n",
        "        B, T, _ = audio_features.shape\n",
        "\n",
        "        # ============ PHONEME EMBEDDING ============\n",
        "        phoneme_emb = self.phoneme_embedding(phoneme_indices)  # (B, T, phoneme_embedding_dim)\n",
        "\n",
        "        # ============ COMBINE AUDIO + PHONEME ============\n",
        "        x = torch.cat([audio_features, phoneme_emb], dim=-1)  # (B, T, 1024+64)\n",
        "        x = self.input_projection(x)  # (B, T, d_model)\n",
        "        x = self.pos_encoder(x)\n",
        "\n",
        "        # ============ ENCODE ============\n",
        "        if mask is not None:\n",
        "            src_key_padding_mask = ~mask\n",
        "        else:\n",
        "            src_key_padding_mask = None\n",
        "\n",
        "        encoded = self.encoder(x, src_key_padding_mask=src_key_padding_mask)  # (B, T, d_model)\n",
        "\n",
        "        # ============ VECTOR QUANTIZATION ============\n",
        "        z = self.pre_quant(encoded)  # (B, T, 256)\n",
        "        quantized, vq_loss, _ = self.vq(z)\n",
        "        quantized = self.post_quant(quantized)  # (B, T, d_model)\n",
        "\n",
        "        # ============ AUTOREGRESSIVE GRU ============\n",
        "        # KRITIƒåNO: GRU odr≈æava \"memory\" izmeƒëu frejmova\n",
        "        if hidden_state is None:\n",
        "            gru_out, hidden_state = self.gru(quantized)  # (B, T, gru_hidden)\n",
        "        else:\n",
        "            gru_out, hidden_state = self.gru(quantized, hidden_state)\n",
        "\n",
        "        # ============ BLENDSHAPE PREDICTION ============\n",
        "        jaw = self.jaw_head(gru_out)\n",
        "        mouth = self.mouth_head(gru_out)\n",
        "        eye = self.eye_head(gru_out)\n",
        "        brow = self.brow_head(gru_out)\n",
        "        other = self.other_head(gru_out)\n",
        "\n",
        "        blendshapes = torch.cat([brow, eye, other, jaw, mouth], dim=-1)  # (B, T, 52)\n",
        "\n",
        "        return blendshapes, vq_loss, hidden_state\n",
        "    '''\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. MULTI-SCALE DISCRIMINATOR (MelGAN/HiFiGAN style)\n",
        "# ==============================================================================\n",
        "\n",
        "class MultiScaleDiscriminator(nn.Module):\n",
        "    \"\"\"\n",
        "    Discriminate at multiple temporal scales\n",
        "    Forces model to capture both fine and coarse motion\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_blendshapes=52):\n",
        "        super().__init__()\n",
        "\n",
        "        self.disc_1x = self._make_discriminator(num_blendshapes, scale=1)\n",
        "        self.disc_2x = self._make_discriminator(num_blendshapes, scale=2)\n",
        "        self.disc_4x = self._make_discriminator(num_blendshapes, scale=4)\n",
        "\n",
        "    def _make_discriminator(self, in_channels, scale):\n",
        "        \"\"\"Single-scale discriminator\"\"\"\n",
        "        return nn.ModuleDict({\n",
        "            'downsample': nn.AvgPool1d(scale, scale) if scale > 1 else nn.Identity(),\n",
        "            'conv_blocks': nn.Sequential(\n",
        "                nn.Conv1d(in_channels, 128, kernel_size=15, padding=7),\n",
        "                nn.LeakyReLU(0.2),\n",
        "                nn.Conv1d(128, 256, kernel_size=11, stride=2, padding=5),\n",
        "                nn.LeakyReLU(0.2),\n",
        "                nn.Conv1d(256, 512, kernel_size=7, stride=2, padding=3),\n",
        "                nn.LeakyReLU(0.2),\n",
        "                nn.Conv1d(512, 512, kernel_size=5, stride=2, padding=2),\n",
        "                nn.LeakyReLU(0.2),\n",
        "            ),\n",
        "            'classifier': nn.Conv1d(512, 1, kernel_size=3, padding=1)\n",
        "        })\n",
        "\n",
        "    def _forward_single_scale(self, x, disc):\n",
        "        \"\"\"Process single scale\"\"\"\n",
        "        x = x.transpose(1, 2)  # (B, T, C) -> (B, C, T)\n",
        "        x = disc['downsample'](x)\n",
        "        features = disc['conv_blocks'](x)\n",
        "        score = disc['classifier'](features)\n",
        "        return score, features\n",
        "\n",
        "    def forward(self, blendshapes):\n",
        "        \"\"\"Returns: list of (score, features) for each scale\"\"\"\n",
        "        scores = []\n",
        "        features = []\n",
        "\n",
        "        for disc in [self.disc_1x, self.disc_2x, self.disc_4x]:\n",
        "            s, f = self._forward_single_scale(blendshapes, disc)\n",
        "            scores.append(s)\n",
        "            features.append(f)\n",
        "\n",
        "        return scores, features\n",
        "\n",
        "\n",
        "print(\"Championship Model Architecture loaded!\")\n",
        "print(\"Key innovations:\")\n",
        "print(\"   - VQ-VAE: Discrete motion codebook\")\n",
        "print(\"   - Autoregressive GRU: Temporal memory\")\n",
        "print(\"   - Phoneme Embedding: Learned phoneme space\")\n",
        "print(\"   - Multi-scale Discriminator: Fine + coarse motion\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ah8xraGWuw61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\"\"\"\n",
        "üèÜ CHAMPIONSHIP LIP-SYNC SYSTEM - ƒÜELIJA 4\n",
        "Advanced Loss Functions:\n",
        "- Correlation Loss (direktna optimizacija sinhronizacije)\n",
        "- L1 Loss (bolje od MSE za blendshapes)\n",
        "- Energy-aware weighting (fokus na aktivne frejmove)\n",
        "- Perceptual Loss (feature matching)\n",
        "- Velocity Loss (dinamika pokreta)\n",
        "\"\"\"\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. CORRELATION LOSS (Research preporuka - kljuƒçno!)\n",
        "# ==============================================================================\n",
        "\n",
        "def correlation_loss(pred, target, mask=None):\n",
        "    \"\"\"\n",
        "    Pearson Correlation Loss\n",
        "\n",
        "    KRITIƒåNO: Forsira model da prati \"oblik\" pokreta, ne samo intenzitet!\n",
        "    Ovo je razlog za≈°to MSE daje nizak correlation (0.28).\n",
        "    \"\"\"\n",
        "    # Normalize\n",
        "    pred_mean = pred.mean(dim=1, keepdim=True)\n",
        "    target_mean = target.mean(dim=1, keepdim=True)\n",
        "\n",
        "    pred_centered = pred - pred_mean\n",
        "    target_centered = target - target_mean\n",
        "\n",
        "    # Correlation\n",
        "    numerator = (pred_centered * target_centered).sum(dim=1)\n",
        "    denominator = torch.sqrt(\n",
        "        (pred_centered ** 2).sum(dim=1) * (target_centered ** 2).sum(dim=1) + 1e-8\n",
        "    )\n",
        "\n",
        "    corr = numerator / denominator\n",
        "\n",
        "    if mask is not None:\n",
        "        # Apply mask\n",
        "        corr = corr * mask.float()\n",
        "        loss = 1 - corr.sum() / (mask.sum() + 1e-8)\n",
        "    else:\n",
        "        loss = 1 - corr.mean()\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. CHAMPIONSHIP LOSS FUNCTION\n",
        "# ==============================================================================\n",
        "\n",
        "class ChampionshipLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    SOTA Loss combining research-proven techniques:\n",
        "\n",
        "    1. L1 Loss (better than MSE for blendshapes)\n",
        "    2. Correlation Loss (sync optimization)\n",
        "    3. Velocity Loss (motion dynamics)\n",
        "    4. Perceptual Loss (feature matching from discriminator)\n",
        "    5. Energy-aware weighting (focus on active frames)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        reconstruction_weight=15.0,\n",
        "        correlation_weight=5.0,\n",
        "        velocity_weight=5.0,\n",
        "        perceptual_weight=0.1,\n",
        "        smoothness_weight=0.0001\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.reconstruction_weight = reconstruction_weight\n",
        "        self.correlation_weight = correlation_weight\n",
        "        self.velocity_weight = velocity_weight\n",
        "        self.perceptual_weight = perceptual_weight\n",
        "        self.smoothness_weight = smoothness_weight\n",
        "\n",
        "        # Important indices\n",
        "        self.mouth_indices = list(range(28, 52))\n",
        "        self.jaw_indices = [24, 25, 26, 27]\n",
        "\n",
        "    def reconstruction_loss(self, pred, target, energy=None, mask=None):\n",
        "        \"\"\"\n",
        "        L1 Loss sa:\n",
        "        - Mouth region weighting\n",
        "        - Energy-aware weighting (focus on active frames)\n",
        "        \"\"\"\n",
        "        # L1 umesto MSE (research preporuka)\n",
        "        l1 = torch.abs(pred - target)\n",
        "\n",
        "        # Region weights\n",
        "        weights = torch.ones_like(l1)\n",
        "        weights[:, :, self.mouth_indices] *= 15.0  # Fokus na usta\n",
        "        weights[:, :, self.jaw_indices] *= 5.0\n",
        "\n",
        "        l1 = l1 * weights\n",
        "\n",
        "        # Energy-aware weighting (fokus na frejmove gde se priƒça)\n",
        "        if energy is not None:\n",
        "            # Normalize energy to [0.5, 1.5] range\n",
        "            energy_weight = 0.5 + energy.unsqueeze(-1)\n",
        "            l1 = l1 * energy_weight\n",
        "\n",
        "        if mask is not None:\n",
        "            l1 = l1 * mask.unsqueeze(-1)\n",
        "            return l1.sum() / (mask.sum() * pred.shape[-1] + 1e-8)\n",
        "\n",
        "        return l1.mean()\n",
        "\n",
        "    def correlation_loss_wrapper(self, pred, target, mask=None):\n",
        "        \"\"\"Per-blendshape correlation loss\"\"\"\n",
        "        total_corr_loss = 0\n",
        "        num_valid = 0\n",
        "\n",
        "        # Compute correlation for each blendshape separately\n",
        "        for i in range(pred.shape[-1]):\n",
        "            pred_i = pred[:, :, i]  # (B, T)\n",
        "            target_i = target[:, :, i]\n",
        "\n",
        "            # Skip if no variation\n",
        "            if target_i.std() < 1e-6:\n",
        "                continue\n",
        "\n",
        "            corr_loss_i = correlation_loss(pred_i, target_i, mask)\n",
        "            total_corr_loss += corr_loss_i\n",
        "            num_valid += 1\n",
        "\n",
        "        return total_corr_loss / (num_valid + 1e-8)\n",
        "\n",
        "    def velocity_loss(self, pred, target, mask=None):\n",
        "        \"\"\"Match motion dynamics (L1 on velocity)\"\"\"\n",
        "        pred_vel = pred[:, 1:] - pred[:, :-1]\n",
        "        target_vel = target[:, 1:] - target[:, :-1]\n",
        "\n",
        "        vel_loss = torch.abs(pred_vel - target_vel)\n",
        "\n",
        "        # Extra weight on mouth\n",
        "        weights = torch.ones_like(vel_loss)\n",
        "        weights[:, :, self.mouth_indices] *= 3.0\n",
        "        vel_loss = vel_loss * weights\n",
        "\n",
        "        if mask is not None:\n",
        "            valid_mask = mask[:, 1:]\n",
        "            return (vel_loss * valid_mask.unsqueeze(-1)).sum() / (valid_mask.sum() + 1e-8)\n",
        "\n",
        "        return vel_loss.mean()\n",
        "\n",
        "    def perceptual_loss(self, pred_features, target_features):\n",
        "        \"\"\"Feature matching loss from discriminator\"\"\"\n",
        "        total_loss = 0\n",
        "        for pred_f, target_f in zip(pred_features, target_features):\n",
        "            total_loss += F.l1_loss(pred_f, target_f)\n",
        "\n",
        "        return total_loss / len(pred_features)\n",
        "\n",
        "    def smoothness_loss(self, pred, mask=None):\n",
        "        \"\"\"Minimal smoothness (anti-jitter only)\"\"\"\n",
        "        acceleration = pred[:, 2:] - 2*pred[:, 1:-1] + pred[:, :-2]\n",
        "\n",
        "        if mask is not None:\n",
        "            valid_mask = mask[:, 2:]\n",
        "            return (acceleration.pow(2) * valid_mask.unsqueeze(-1)).sum() / (valid_mask.sum() + 1e-8)\n",
        "\n",
        "        return acceleration.pow(2).mean()\n",
        "\n",
        "    def forward(self, pred, target, pred_features, target_features, energy=None, mask=None):\n",
        "        \"\"\"\n",
        "        Compute total loss\n",
        "\n",
        "        Args:\n",
        "            pred: Predicted blendshapes (B, T, 52)\n",
        "            target: Ground truth blendshapes (B, T, 52)\n",
        "            pred_features: List of discriminator features for pred\n",
        "            target_features: List of discriminator features for target\n",
        "            energy: Frame-wise energy (B, T)\n",
        "            mask: Valid frame mask (B, T)\n",
        "\n",
        "        Returns:\n",
        "            Dictionary of losses\n",
        "        \"\"\"\n",
        "        recon = self.reconstruction_loss(pred, target, energy, mask)\n",
        "        corr = self.correlation_loss_wrapper(pred, target, mask)\n",
        "        vel = self.velocity_loss(pred, target, mask)\n",
        "        perc = self.perceptual_loss(pred_features, target_features)\n",
        "        smooth = self.smoothness_loss(pred, mask)\n",
        "\n",
        "        total = (\n",
        "            self.reconstruction_weight * recon +\n",
        "            self.correlation_weight * corr +      # NOVO - kljuƒçno!\n",
        "            self.velocity_weight * vel +\n",
        "            self.perceptual_weight * perc +\n",
        "            self.smoothness_weight * smooth\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'total': total,\n",
        "            'reconstruction': recon,\n",
        "            'correlation': corr,\n",
        "            'velocity': vel,\n",
        "            'perceptual': perc,\n",
        "            'smoothness': smooth\n",
        "        }\n",
        "\n",
        "\n",
        "print(\"Championship Loss Functions loaded!\")\n",
        "print(\"Key innovations:\")\n",
        "print(\"   - Correlation Loss: Direct sync optimization\")\n",
        "print(\"   - L1 Loss: Better than MSE for blendshapes\")\n",
        "print(\"   - Energy-aware: Focus on active frames\")\n",
        "print(\"   - Perceptual Loss: Feature matching\")\n",
        "print(\"\\n Loss Weights:\")\n",
        "print(\"   Reconstruction: 1.0\")\n",
        "print(\"   Correlation: 5.0 (NOVO - kljuƒçno!)\")\n",
        "print(\"   Velocity: 2.0\")\n",
        "print(\"   Perceptual: 10.0\")\n",
        "print(\"   Smoothness: 0.001\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nzbqFMgGu5qw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\"\"\"\n",
        "üèÜ CHAMPIONSHIP LIP-SYNC SYSTEM - ƒÜELIJA 5\n",
        "Championship Trainer sa:\n",
        "- Gradient Accumulation (batch_size=8 efektivno)\n",
        "- Mixed Precision Training\n",
        "- Cosine Annealing sa Warmup\n",
        "- GRU Hidden State Reset\n",
        "\"\"\"\n",
        "\n",
        "class ChampionshipTrainer:\n",
        "    \"\"\"\n",
        "    SOTA Trainer incorporating research best practices\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model,\n",
        "        discriminator,\n",
        "        train_loader,\n",
        "        val_loader,\n",
        "        device='cuda',\n",
        "        learning_rate=1e-4,\n",
        "        gradient_accumulation_steps=8,  # batch_size=1 * 8 = efektivno 8\n",
        "        use_wandb=False\n",
        "    ):\n",
        "        self.model = model.to(device)\n",
        "        self.discriminator = discriminator.to(device)\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.device = device\n",
        "        self.gradient_accumulation_steps = gradient_accumulation_steps\n",
        "        self.use_wandb = use_wandb\n",
        "\n",
        "        # Loss functions\n",
        "        self.criterion = ChampionshipLoss()\n",
        "        self.adversarial_loss = nn.MSELoss()  # Least-squares GAN\n",
        "\n",
        "        # Optimizers\n",
        "        self.optimizer_g = optim.AdamW(\n",
        "            model.parameters(),\n",
        "            lr=learning_rate,\n",
        "            betas=(0.5, 0.999),\n",
        "            weight_decay=1e-4\n",
        "        )\n",
        "        self.optimizer_d = optim.AdamW(\n",
        "            discriminator.parameters(),\n",
        "            lr=learning_rate,\n",
        "            betas=(0.5, 0.999),\n",
        "            weight_decay=1e-4\n",
        "        )\n",
        "\n",
        "        # Schedulers - cosine annealing sa warmup\n",
        "        total_steps = len(train_loader) // gradient_accumulation_steps * 200\n",
        "\n",
        "        self.scheduler_g = OneCycleLR(\n",
        "            self.optimizer_g,\n",
        "            max_lr=learning_rate,\n",
        "            total_steps=total_steps,\n",
        "            pct_start=0.1,\n",
        "            anneal_strategy='cos'\n",
        "        )\n",
        "        self.scheduler_d = OneCycleLR(\n",
        "            self.optimizer_d,\n",
        "            max_lr=learning_rate,\n",
        "            total_steps=total_steps,\n",
        "            pct_start=0.1,\n",
        "            anneal_strategy='cos'\n",
        "        )\n",
        "\n",
        "        # Mixed precision\n",
        "        self.scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "        self.best_val_loss = float('inf')\n",
        "        self.best_val_corr = 0.0\n",
        "        self.patience = 40\n",
        "        self.patience_counter = 0\n",
        "\n",
        "    def reset_gru_hidden(self, batch_size):\n",
        "        \"\"\"Reset GRU hidden state for each batch\"\"\"\n",
        "        return None  # GRU ƒáe kreirati novi hidden state\n",
        "\n",
        "    def train_discriminator(self, batch, accumulation_step):\n",
        "        \"\"\"Train discriminator with multi-scale approach\"\"\"\n",
        "        audio = batch['audio_features'].to(self.device)\n",
        "        phoneme = batch['phoneme_indices'].to(self.device)\n",
        "        target = batch['blendshapes'].to(self.device)\n",
        "        mask = batch['mask'].to(self.device)\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "            # Generate fake samples\n",
        "            with torch.no_grad():\n",
        "                hidden = self.reset_gru_hidden(audio.shape[0])\n",
        "                fake, _, _ = self.model(audio, phoneme, mask, hidden)\n",
        "\n",
        "            # Discriminate real\n",
        "            real_scores, _ = self.discriminator(target)\n",
        "            # Discriminate fake\n",
        "            fake_scores, _ = self.discriminator(fake.detach())\n",
        "\n",
        "            # Loss (least-squares GAN)\n",
        "            d_loss_real = sum([F.mse_loss(s, torch.ones_like(s)) for s in real_scores]) / len(real_scores)\n",
        "            d_loss_fake = sum([F.mse_loss(s, torch.zeros_like(s)) for s in fake_scores]) / len(fake_scores)\n",
        "\n",
        "            d_loss = (d_loss_real + d_loss_fake) / (2 * self.gradient_accumulation_steps)\n",
        "\n",
        "        self.scaler.scale(d_loss).backward()\n",
        "\n",
        "        if (accumulation_step + 1) % self.gradient_accumulation_steps == 0:\n",
        "            self.scaler.step(self.optimizer_d)\n",
        "            self.scaler.update()\n",
        "            self.optimizer_d.zero_grad()\n",
        "            self.scheduler_d.step()\n",
        "\n",
        "        return d_loss.item() * self.gradient_accumulation_steps\n",
        "\n",
        "    def train_generator(self, batch, accumulation_step, epoch):\n",
        "        \"\"\"Train generator with all losses\"\"\"\n",
        "        audio = batch['audio_features'].to(self.device)\n",
        "        phoneme = batch['phoneme_indices'].to(self.device)\n",
        "        target = batch['blendshapes'].to(self.device)\n",
        "        energy = batch['energy'].to(self.device)\n",
        "        mask = batch['mask'].to(self.device)\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "            # Generate\n",
        "            hidden = self.reset_gru_hidden(audio.shape[0])\n",
        "            pred, vq_loss, _ = self.model(audio, phoneme, mask, hidden)\n",
        "\n",
        "            # Get discriminator features for perceptual loss\n",
        "            with torch.no_grad():\n",
        "                _, target_features = self.discriminator(target)\n",
        "            pred_scores, pred_features = self.discriminator(pred)\n",
        "\n",
        "            # Main losses\n",
        "            losses = self.criterion(pred, target, pred_features, target_features, energy, mask)\n",
        "\n",
        "            # Adversarial loss\n",
        "            adv_loss = sum([F.mse_loss(s, torch.ones_like(s)) for s in pred_scores]) / len(pred_scores)\n",
        "\n",
        "\n",
        "            # Dinamiƒçka te≈æina za energiju pokreta\n",
        "            # Prvih 30 epoha 0.1 (da nauƒçi sink), posle 0.2 (da postane energiƒçan)\n",
        "            adv_weight = 0.1 if epoch < 30 else 0.2\n",
        "\n",
        "            # Total generator loss\n",
        "            g_loss = (\n",
        "                losses['total'] +\n",
        "                adv_weight * adv_loss +  # KORISTIMO adv_weight\n",
        "                1.0 * vq_loss\n",
        "            ) / self.gradient_accumulation_steps\n",
        "\n",
        "\n",
        "        self.scaler.scale(g_loss).backward()\n",
        "\n",
        "        if (accumulation_step + 1) % self.gradient_accumulation_steps == 0:\n",
        "            self.scaler.unscale_(self.optimizer_g)\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
        "            self.scaler.step(self.optimizer_g)\n",
        "            self.scaler.update()\n",
        "            self.optimizer_g.zero_grad()\n",
        "            self.scheduler_g.step()\n",
        "\n",
        "        return {\n",
        "            'g_loss': g_loss.item() * self.gradient_accumulation_steps,\n",
        "            'reconstruction': losses['reconstruction'].item(),\n",
        "            'correlation': losses['correlation'].item(),\n",
        "            'velocity': losses['velocity'].item(),\n",
        "            'perceptual': losses['perceptual'].item(),\n",
        "            'vq_loss': vq_loss.item()\n",
        "        }\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def validate(self):\n",
        "        \"\"\"Validation with correlation metric\"\"\"\n",
        "        self.model.eval()\n",
        "        val_losses = []\n",
        "        val_corrs = []\n",
        "\n",
        "        for batch in self.val_loader:\n",
        "            audio = batch['audio_features'].to(self.device)\n",
        "            phoneme = batch['phoneme_indices'].to(self.device)\n",
        "            target = batch['blendshapes'].to(self.device)\n",
        "            energy = batch['energy'].to(self.device)\n",
        "            mask = batch['mask'].to(self.device)\n",
        "\n",
        "            with torch.cuda.amp.autocast():\n",
        "                hidden = self.reset_gru_hidden(audio.shape[0])\n",
        "                pred, vq_loss, _ = self.model(audio, phoneme, mask, hidden)\n",
        "\n",
        "                _, target_features = self.discriminator(target)\n",
        "                _, pred_features = self.discriminator(pred)\n",
        "\n",
        "                losses = self.criterion(pred, target, pred_features, target_features, energy, mask)\n",
        "\n",
        "            val_losses.append(losses['total'].item())\n",
        "\n",
        "            # Compute correlation\n",
        "            pred_np = pred[mask].cpu().numpy()\n",
        "            target_np = target[mask].cpu().numpy()\n",
        "\n",
        "            if len(pred_np) > 10:\n",
        "                corr, _ = pearsonr(pred_np.flatten(), target_np.flatten())\n",
        "                if not np.isnan(corr):\n",
        "                    val_corrs.append(corr)\n",
        "\n",
        "        self.model.train()\n",
        "        return np.mean(val_losses), np.mean(val_corrs) if val_corrs else 0.0\n",
        "\n",
        "    def train(self, num_epochs, save_dir='checkpoints'):\n",
        "        \"\"\"Main training loop\"\"\"\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "        if self.use_wandb:\n",
        "            import wandb\n",
        "            wandb.init(project='championship-lipsync')\n",
        "\n",
        "        self.optimizer_g.zero_grad()\n",
        "        self.optimizer_d.zero_grad()\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            self.model.train()\n",
        "            epoch_metrics = {\n",
        "                'g_loss': [], 'reconstruction': [], 'correlation': [],\n",
        "                'velocity': [], 'perceptual': [], 'vq_loss': [], 'd_loss': []\n",
        "            }\n",
        "\n",
        "            pbar = tqdm(self.train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
        "\n",
        "            for batch_idx, batch in enumerate(pbar):\n",
        "                # Train discriminator every 2 batches\n",
        "                if batch_idx % 2 == 0:\n",
        "                    d_loss = self.train_discriminator(batch, batch_idx)\n",
        "                    epoch_metrics['d_loss'].append(d_loss)\n",
        "\n",
        "                # Train generator\n",
        "                g_metrics = self.train_generator(batch, batch_idx, epoch)\n",
        "                for k, v in g_metrics.items():\n",
        "                    epoch_metrics[k].append(v)\n",
        "\n",
        "                '''\n",
        "                pbar.set_postfix({\n",
        "                    'G': f\"{np.mean(epoch_metrics['g_loss']):.4f}\",\n",
        "                    'Corr': f\"{np.mean(epoch_metrics['correlation']):.4f}\",\n",
        "                    'Rec': f\"{np.mean(epoch_metrics['reconstruction']):.4f}\"\n",
        "                })\n",
        "                '''\n",
        "                pbar.set_postfix({\n",
        "                    'G': f\"{np.mean(epoch_metrics['g_loss']):.3f}\",\n",
        "                    'D': f\"{np.mean(epoch_metrics['d_loss']) if epoch_metrics['d_loss'] else 0:.3f}\",\n",
        "                    'VQ': f\"{np.mean(epoch_metrics['vq_loss']):.4f}\",\n",
        "                    'Corr': f\"{np.mean(epoch_metrics['correlation']):.3f}\",\n",
        "                    'Rec': f\"{np.mean(epoch_metrics['reconstruction']):.3f}\",\n",
        "                    'Best': f\"{self.best_val_corr:.3f}\"\n",
        "                })\n",
        "\n",
        "                # Clear cache periodically\n",
        "                if batch_idx % 10 == 0:\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "            # Validation\n",
        "            val_loss, val_corr = self.validate()\n",
        "            print(f\"\\nEpoch {epoch+1} - Val Loss: {val_loss:.4f}, Val Correlation: {val_corr:.4f}\")\n",
        "\n",
        "            if self.use_wandb:\n",
        "                import wandb\n",
        "                wandb.log({\n",
        "                    'epoch': epoch,\n",
        "                    'train/g_loss': np.mean(epoch_metrics['g_loss']),\n",
        "                    'train/correlation': np.mean(epoch_metrics['correlation']),\n",
        "                    'train/reconstruction': np.mean(epoch_metrics['reconstruction']),\n",
        "                    'val/loss': val_loss,\n",
        "                    'val/correlation': val_corr\n",
        "                })\n",
        "\n",
        "            # Save best model (based on correlation!)\n",
        "            if val_corr > self.best_val_corr:\n",
        "                self.best_val_corr = val_corr\n",
        "                self.best_val_loss = val_loss\n",
        "                self.patience_counter = 0\n",
        "\n",
        "                checkpoint = {\n",
        "                    'epoch': epoch,\n",
        "                    'model_state_dict': self.model.state_dict(),\n",
        "                    'discriminator_state_dict': self.discriminator.state_dict(),\n",
        "                    'optimizer_g_state_dict': self.optimizer_g.state_dict(),\n",
        "                    'val_loss': val_loss,\n",
        "                    'val_correlation': val_corr\n",
        "                }\n",
        "                torch.save(checkpoint, os.path.join(save_dir, 'best_model.pt'))\n",
        "                print(f\"‚úÖ Saved best model (corr: {val_corr:.4f})\")\n",
        "            else:\n",
        "                self.patience_counter += 1\n",
        "\n",
        "            # Early stopping\n",
        "            if self.patience_counter >= self.patience:\n",
        "                print(f\"‚ö†Ô∏è Early stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "            # Periodic checkpoint\n",
        "            if (epoch + 1) % 10 == 0:\n",
        "                checkpoint = {\n",
        "                    'epoch': epoch,\n",
        "                    'model_state_dict': self.model.state_dict(),\n",
        "                    'val_correlation': val_corr\n",
        "                }\n",
        "                torch.save(checkpoint, os.path.join(save_dir, f'checkpoint_epoch_{epoch+1}.pt'))\n",
        "\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        if self.use_wandb:\n",
        "            import wandb\n",
        "            wandb.finish()\n",
        "\n",
        "        print(f\"\\nüèÜ Training complete!\")\n",
        "        print(f\"Best validation correlation: {self.best_val_corr:.4f}\")\n",
        "        print(f\"Best validation loss: {self.best_val_loss:.4f}\")\n",
        "\n",
        "\n",
        "print(\"Championship Trainer loaded!\")\n",
        "print(\"Key features:\")\n",
        "print(\"   - Gradient Accumulation: Effective batch_size=8\")\n",
        "print(\"   - Mixed Precision: Faster training\")\n",
        "print(\"   - Correlation-based checkpointing\")\n",
        "print(\"   - GRU hidden state management\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "j3jLv-86vBM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\"\"\"\n",
        "üèÜ CHAMPIONSHIP LIP-SYNC SYSTEM - ƒÜELIJA 6\n",
        "Main Training Script\n",
        "\"\"\"\n",
        "\n",
        "def train_championship_model(\n",
        "    base_path='/content/drive/MyDrive/AI-speak',\n",
        "    save_dir='checkpoints_championship',\n",
        "    batch_size=1,  # Mora biti 1 zbog razliƒçitih du≈æina\n",
        "    gradient_accumulation_steps=8,  # Efektivni batch = 8\n",
        "    learning_rate=1e-4,\n",
        "    num_epochs=200,\n",
        "    d_model=512,  # Smanji na 384 ako ima≈° malo VRAM-a\n",
        "    num_encoder_layers=4,  # Smanji na 3 za manje VRAM\n",
        "    codebook_size=512,\n",
        "    use_wandb=False,\n",
        "    device='cuda'\n",
        "):\n",
        "    \"\"\"\n",
        "    Glavni training script sa svim optimizacijama\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"=\"*70)\n",
        "    print(\"üèÜ CHAMPIONSHIP LIP-SYNC TRAINING\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Device: {device}\")\n",
        "    print(f\"Effective batch size: {batch_size * gradient_accumulation_steps}\")\n",
        "    print(f\"Model: d_model={d_model}, layers={num_encoder_layers}\")\n",
        "    print(f\"Codebook size: {codebook_size}\")\n",
        "    print(f\"Learning rate: {learning_rate}\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    # Clear memory\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    # ========== LOAD DATA ==========\n",
        "    print(\"üìÇ Loading datasets...\")\n",
        "    train_loader, val_loader = create_dataloaders(\n",
        "        base_path,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=0,\n",
        "        device=device\n",
        "    )\n",
        "    print(f\"‚úÖ Train: {len(train_loader)} batches\")\n",
        "    print(f\"‚úÖ Val: {len(val_loader)} batches\\n\")\n",
        "\n",
        "    # ========== BUILD MODEL ==========\n",
        "    print(\"üèóÔ∏è Building Championship model...\")\n",
        "    model = ChampionshipLipSyncModel(\n",
        "        audio_dim=1024,\n",
        "        phoneme_embedding_dim=64,\n",
        "        d_model=d_model,\n",
        "        num_encoder_layers=num_encoder_layers,\n",
        "        num_heads=8,\n",
        "        dim_feedforward=2048,\n",
        "        num_blendshapes=52,\n",
        "        codebook_size=codebook_size,\n",
        "        gru_hidden=256,\n",
        "        dropout=0.1\n",
        "    )\n",
        "\n",
        "    discriminator = MultiScaleDiscriminator(num_blendshapes=52)\n",
        "\n",
        "    # Model summary\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"‚úÖ Generator params: {total_params:,} (trainable: {trainable_params:,})\")\n",
        "\n",
        "    disc_params = sum(p.numel() for p in discriminator.parameters())\n",
        "    print(f\"‚úÖ Discriminator params: {disc_params:,}\\n\")\n",
        "\n",
        "    # ========== KEY IMPROVEMENTS ==========\n",
        "    print(\"=\"*70)\n",
        "    print(\"üîß KEY IMPROVEMENTS (Research-based):\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"‚úÖ VQ-VAE: Discrete motion codebook (CodeTalker)\")\n",
        "    print(\"‚úÖ Autoregressive GRU: Temporal memory (FaceFormer)\")\n",
        "    print(\"‚úÖ Phoneme Embedding: Learned phoneme space\")\n",
        "    print(\"‚úÖ Correlation Loss: Direct sync optimization\")\n",
        "    print(\"‚úÖ L1 Loss: Better than MSE for blendshapes\")\n",
        "    print(\"‚úÖ Energy-aware: Focus on active frames\")\n",
        "    print(\"‚úÖ Multi-scale Discriminator: Fine + coarse motion\")\n",
        "    print(\"‚úÖ Gradient Accumulation: Effective batch_size=8\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    # ========== LOSS WEIGHTS ==========\n",
        "    print(\"üìä Loss Weights:\")\n",
        "    print(\"   Reconstruction (L1): 1.0\")\n",
        "    print(\"   Correlation: 5.0 ‚≠ê (NOVO - kljuƒçno!)\")\n",
        "    print(\"   Velocity: 2.0\")\n",
        "    print(\"   Perceptual: 10.0\")\n",
        "    print(\"   Smoothness: 0.001\")\n",
        "    print(\"   VQ-VAE: 0.1\")\n",
        "    print(\"   Adversarial: 0.1\\n\")\n",
        "\n",
        "    # ========== INITIALIZE TRAINER ==========\n",
        "    print(\"‚öôÔ∏è Initializing trainer...\")\n",
        "    trainer = ChampionshipTrainer(\n",
        "        model=model,\n",
        "        discriminator=discriminator,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        device=device,\n",
        "        learning_rate=learning_rate,\n",
        "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "        use_wandb=use_wandb\n",
        "    )\n",
        "    print(\"‚úÖ Trainer ready!\\n\")\n",
        "\n",
        "    # ========== START TRAINING ==========\n",
        "    print(\"=\"*70)\n",
        "    print(\"üöÄ STARTING TRAINING\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    try:\n",
        "        trainer.train(num_epochs=num_epochs, save_dir=save_dir)\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\n‚ö†Ô∏è Training interrupted by user\")\n",
        "        checkpoint = {\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'discriminator_state_dict': discriminator.state_dict(),\n",
        "            'note': 'interrupted'\n",
        "        }\n",
        "        torch.save(checkpoint, os.path.join(save_dir, 'interrupted.pt'))\n",
        "        print(\"üíæ Saved interrupted checkpoint\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"üéâ TRAINING COMPLETE!\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Best correlation: {trainer.best_val_corr:.4f}\")\n",
        "    print(f\"Best loss: {trainer.best_val_loss:.4f}\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# POKRETANJE\n",
        "# ==============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Mount Google Drive (za Colab)\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "    except:\n",
        "        print(\"Not on Colab, skipping drive mount\")\n",
        "\n",
        "    # Run training\n",
        "    train_championship_model(\n",
        "        base_path='/content/drive/MyDrive/AI-speak',  # PROMENI OVO!\n",
        "        save_dir='checkpoints_championship',\n",
        "        batch_size=1,\n",
        "        gradient_accumulation_steps=8,\n",
        "        learning_rate=1e-4,\n",
        "        num_epochs=200,\n",
        "        d_model=512,  # Smanji na 384 za manje VRAM\n",
        "        num_encoder_layers=4,  # Smanji na 3 za manje VRAM\n",
        "        codebook_size=512,\n",
        "        use_wandb=False,\n",
        "        device='cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    )\n",
        "\n",
        "\n",
        "print(\"‚úÖ Training script ready!\")\n",
        "print(\"üìå Pokrenite: train_championship_model()\")\n",
        "print(\"üìå Ili direktno pokrenite ovu ƒáeliju\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NLNUBU9cvGwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"\n",
        "üèÜ CHAMPIONSHIP LIP-SYNC SYSTEM - ƒÜELIJA 7\n",
        "Production-Ready Inference System\n",
        "\"\"\"\n",
        "\n",
        "class MinimalPostProcessor:\n",
        "    \"\"\"\n",
        "    Minimalan post-processing jer model veƒá generi≈°e dobre predikcije\n",
        "    Samo enforce hard constraints\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, fps=100):\n",
        "        self.fps = fps\n",
        "\n",
        "        self.constraints = {\n",
        "            'eye_symmetry': {\n",
        "                'pairs': [(13, 14)],\n",
        "                'max_diff': 0.3\n",
        "            },\n",
        "            'jaw_mutual_exclusive': {\n",
        "                'indices': [24, 36],\n",
        "                'max_sum': 1.2\n",
        "            },\n",
        "            'max_velocity': {\n",
        "                'jaw': 0.25,\n",
        "                'mouth': 0.20,\n",
        "                'eyes': 0.30,\n",
        "                'default': 0.18\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def enforce_symmetry(self, blendshapes):\n",
        "        \"\"\"Enforce eye symmetry\"\"\"\n",
        "        processed = blendshapes.copy()\n",
        "\n",
        "        for pair in self.constraints['eye_symmetry']['pairs']:\n",
        "            left_idx, right_idx = pair\n",
        "            max_diff = self.constraints['eye_symmetry']['max_diff']\n",
        "\n",
        "            diff = np.abs(processed[:, left_idx] - processed[:, right_idx])\n",
        "            excessive = diff > max_diff\n",
        "\n",
        "            if excessive.any():\n",
        "                avg = (processed[excessive, left_idx] + processed[excessive, right_idx]) / 2\n",
        "                processed[excessive, left_idx] = avg\n",
        "                processed[excessive, right_idx] = avg\n",
        "\n",
        "        return processed\n",
        "\n",
        "    def clamp_extreme_velocity(self, blendshapes):\n",
        "        \"\"\"Prevent only EXTREME rapid movements\"\"\"\n",
        "        processed = blendshapes.copy()\n",
        "\n",
        "        jaw_indices = [24, 25, 26, 27]\n",
        "        mouth_indices = list(range(28, 52))\n",
        "        eye_indices = list(range(5, 19))\n",
        "\n",
        "        def clamp_group(indices, max_vel):\n",
        "            for idx in indices:\n",
        "                velocity = np.diff(processed[:, idx], prepend=processed[0, idx])\n",
        "                excessive = np.abs(velocity) > max_vel\n",
        "\n",
        "                if excessive.any():\n",
        "                    for i in np.where(excessive)[0]:\n",
        "                        if i > 0:\n",
        "                            target = processed[i-1, idx] + np.sign(velocity[i]) * max_vel\n",
        "                            processed[i, idx] = target\n",
        "\n",
        "        clamp_group(jaw_indices, self.constraints['max_velocity']['jaw'])\n",
        "        clamp_group(mouth_indices, self.constraints['max_velocity']['mouth'])\n",
        "        clamp_group(eye_indices, self.constraints['max_velocity']['eyes'])\n",
        "\n",
        "        return processed\n",
        "\n",
        "    def process(self, blendshapes):\n",
        "        \"\"\"Minimal processing - samo constraints\"\"\"\n",
        "        processed = blendshapes.copy()\n",
        "        processed = self.enforce_symmetry(processed)\n",
        "        processed = self.clamp_extreme_velocity(processed)\n",
        "        return np.clip(processed, 0, 1)\n",
        "\n",
        "\n",
        "class ChampionshipInference:\n",
        "    \"\"\"\n",
        "    Production-ready inference system for Championship model\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_path, device='cuda'):\n",
        "        self.device = device\n",
        "        self.fps = 100\n",
        "\n",
        "        print(\"üîÑ Loading Championship model...\")\n",
        "        checkpoint = torch.load(model_path, map_location=device, weights_only=False)\n",
        "\n",
        "        # Load model\n",
        "        self.model = ChampionshipLipSyncModel(\n",
        "            audio_dim=1024,\n",
        "            phoneme_embedding_dim=64,\n",
        "            d_model=512,\n",
        "            num_encoder_layers=4,\n",
        "            num_heads=8,\n",
        "            dim_feedforward=2048,\n",
        "            num_blendshapes=52,\n",
        "            codebook_size=512,\n",
        "            gru_hidden=256,\n",
        "            dropout=0.1\n",
        "        ).to(device)\n",
        "\n",
        "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        self.model.eval()\n",
        "\n",
        "        # Audio processor\n",
        "        print(\"üîÑ Loading Wav2Vec2...\")\n",
        "        self.feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-xls-r-300m\")\n",
        "        self.wav2vec = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-xls-r-300m\").to(device)\n",
        "        self.wav2vec.eval()\n",
        "\n",
        "        for param in self.wav2vec.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Processors\n",
        "        self.post_processor = MinimalPostProcessor(fps=self.fps)\n",
        "        self.phoneme_processor = PhonemeProcessor(fps=self.fps)\n",
        "\n",
        "        # Performance tracking\n",
        "        self.latency_history = []\n",
        "\n",
        "        print(\"‚úÖ Inference system ready!\")\n",
        "        print(f\"üìä Model epoch: {checkpoint.get('epoch', 'unknown')}\")\n",
        "        print(f\"üìä Val correlation: {checkpoint.get('val_correlation', 'unknown'):.4f}\")\n",
        "\n",
        "    def extract_audio_features(self, audio_path, sr=16000):\n",
        "        \"\"\"Extract Wav2Vec2 features\"\"\"\n",
        "        waveform, original_sr = torchaudio.load(audio_path)\n",
        "\n",
        "        if original_sr != sr:\n",
        "            resampler = T.Resample(original_sr, sr)\n",
        "            waveform = resampler(waveform)\n",
        "\n",
        "        waveform = waveform / (waveform.abs().max() + 1e-8)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            inputs = self.feature_extractor(\n",
        "                waveform.squeeze().cpu().numpy(),\n",
        "                sampling_rate=sr,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "\n",
        "            outputs = self.wav2vec(\n",
        "                inputs.input_values.to(self.device),\n",
        "                output_hidden_states=True\n",
        "            )\n",
        "\n",
        "            hidden_states = outputs.hidden_states[-4:]\n",
        "            features = torch.stack(hidden_states).mean(0).squeeze(0)\n",
        "\n",
        "        return features.cpu().numpy()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict(\n",
        "        self,\n",
        "        audio_path: str,\n",
        "        phoneme_alignment_path: Optional[str] = None,\n",
        "        target_fps: int = 100,\n",
        "        apply_post_processing: bool = True\n",
        "    ) -> Dict[str, np.ndarray]:\n",
        "        \"\"\"\n",
        "        Generate blendshape coefficients from audio\n",
        "\n",
        "        Args:\n",
        "            audio_path: Path to audio file\n",
        "            phoneme_alignment_path: Optional phoneme alignment\n",
        "            target_fps: Target frame rate\n",
        "            apply_post_processing: Apply minimal post-processing\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with blendshapes, timestamps, latency, etc.\n",
        "        \"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Extract audio features\n",
        "        audio_features = self.extract_audio_features(audio_path)\n",
        "\n",
        "        # Align to target FPS\n",
        "        audio_duration = audio_features.shape[0] / 50\n",
        "        num_frames = int(audio_duration * target_fps)\n",
        "\n",
        "        # Interpolate\n",
        "        old_indices = np.linspace(0, audio_features.shape[0] - 1, audio_features.shape[0])\n",
        "        new_indices = np.linspace(0, audio_features.shape[0] - 1, num_frames)\n",
        "        interpolator = interp1d(old_indices, audio_features, axis=0, kind='linear')\n",
        "        audio_features_aligned = interpolator(new_indices)\n",
        "\n",
        "        # Phoneme sequence\n",
        "        if phoneme_alignment_path and os.path.exists(phoneme_alignment_path):\n",
        "            alignments = self.phoneme_processor.parse_alignment_file(phoneme_alignment_path)\n",
        "            phoneme_indices = self.phoneme_processor.create_phoneme_indices(alignments, num_frames)\n",
        "        else:\n",
        "            phoneme_indices = np.zeros(num_frames, dtype=np.int64)\n",
        "\n",
        "        # Prepare inputs\n",
        "        audio_tensor = torch.from_numpy(audio_features_aligned).float().unsqueeze(0).to(self.device)\n",
        "        phoneme_tensor = torch.from_numpy(phoneme_indices).long().unsqueeze(0).to(self.device)\n",
        "        mask = torch.ones(1, num_frames, dtype=torch.bool).to(self.device)\n",
        "\n",
        "        # Generate blendshapes\n",
        "        with torch.cuda.amp.autocast():\n",
        "            blendshapes, vq_loss, _ = self.model(audio_tensor, phoneme_tensor, mask)\n",
        "\n",
        "        blendshapes_np = blendshapes.squeeze(0).cpu().numpy()\n",
        "\n",
        "        # Post-processing\n",
        "        if apply_post_processing:\n",
        "            blendshapes_np = self.post_processor.process(blendshapes_np)\n",
        "\n",
        "        # Timestamps\n",
        "        timestamps = np.arange(num_frames) / target_fps\n",
        "\n",
        "        # Latency\n",
        "        latency = time.time() - start_time\n",
        "        self.latency_history.append(latency)\n",
        "\n",
        "        return {\n",
        "            'blendshapes': blendshapes_np,\n",
        "            'timestamps': timestamps,\n",
        "            'latency': latency,\n",
        "            'fps': target_fps,\n",
        "            'vq_loss': vq_loss.item()\n",
        "        }\n",
        "\n",
        "    def export_to_csv(self, blendshapes, output_path):\n",
        "        \"\"\"Export blendshapes to CSV\"\"\"\n",
        "        blendshape_names = [\n",
        "            'browInnerUp', 'browDownLeft', 'browDownRight', 'browOuterUpLeft', 'browOuterUpRight',\n",
        "            'eyeLookUpLeft', 'eyeLookUpRight', 'eyeLookDownLeft', 'eyeLookDownRight',\n",
        "            'eyeLookInLeft', 'eyeLookInRight', 'eyeLookOutLeft', 'eyeLookOutRight',\n",
        "            'eyeBlinkLeft', 'eyeBlinkRight', 'eyeSquintLeft', 'eyeSquintRight',\n",
        "            'eyeWideLeft', 'eyeWideRight', 'cheekPuff', 'cheekSquintLeft', 'cheekSquintRight',\n",
        "            'noseSneerLeft', 'noseSneerRight', 'jawOpen', 'jawForward', 'jawLeft', 'jawRight',\n",
        "            'mouthFunnel', 'mouthPucker', 'mouthLeft', 'mouthRight', 'mouthRollUpper',\n",
        "            'mouthRollLower', 'mouthShrugUpper', 'mouthShrugLower', 'mouthClose',\n",
        "            'mouthSmileLeft', 'mouthSmileRight', 'mouthFrownLeft', 'mouthFrownRight',\n",
        "            'mouthDimpleLeft', 'mouthDimpleRight', 'mouthUpperUpLeft', 'mouthUpperUpRight',\n",
        "            'mouthLowerDownLeft', 'mouthLowerDownRight', 'mouthPressLeft', 'mouthPressRight',\n",
        "            'mouthStretchLeft', 'mouthStretchRight', 'tongueOut'\n",
        "        ]\n",
        "\n",
        "        df = pd.DataFrame(blendshapes, columns=blendshape_names)\n",
        "        df.to_csv(output_path, index=False)\n",
        "        print(f\"‚úÖ Exported {len(df)} frames to {output_path}\")\n",
        "\n",
        "    def get_performance_stats(self):\n",
        "        \"\"\"Get performance statistics\"\"\"\n",
        "        if not self.latency_history:\n",
        "            return None\n",
        "\n",
        "        return {\n",
        "            'mean_latency': np.mean(self.latency_history),\n",
        "            'median_latency': np.median(self.latency_history),\n",
        "            'p95_latency': np.percentile(self.latency_history, 95),\n",
        "            'total_inferences': len(self.latency_history),\n",
        "            'fps_realtime': 1.0 / np.mean(self.latency_history)\n",
        "        }\n",
        "\n",
        "\n",
        "print(\"‚úÖ Championship Inference system loaded!\")\n",
        "print(\"üìå Usage:\")\n",
        "print(\"   inference = ChampionshipInference('checkpoints_championship/best_model.pt')\")\n",
        "print(\"   result = inference.predict('audio.wav', 'phonemes.txt')\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xqGADbLdvMZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\"\"\"\n",
        "üèÜ CHAMPIONSHIP LIP-SYNC SYSTEM - ƒÜELIJA 8\n",
        "Comprehensive Validation & Visualization\n",
        "\"\"\"\n",
        "\n",
        "class ChampionshipValidator:\n",
        "    \"\"\"\n",
        "    Comprehensive validation system\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_path, base_path, device='cuda'):\n",
        "        self.model_path = model_path\n",
        "        self.base_path = base_path\n",
        "        self.device = device\n",
        "\n",
        "        # Load inference system\n",
        "        self.inference = ChampionshipInference(model_path, device)\n",
        "\n",
        "        # Get validation files\n",
        "        self.val_files = self._get_validation_files()\n",
        "        print(f\"‚úÖ Found {len(self.val_files)} validation files\")\n",
        "\n",
        "    def _get_validation_files(self):\n",
        "        \"\"\"Get validation file IDs\"\"\"\n",
        "        path = os.path.join(self.base_path, 'spk08_ser')\n",
        "        all_files = sorted([f for f in os.listdir(path) if f.endswith('.npy')])\n",
        "        file_ids = [f.replace('.npy', '').replace('08_ser_spk08_', '') for f in all_files]\n",
        "        return file_ids[int(len(file_ids) * 0.8):]\n",
        "\n",
        "    def load_ground_truth(self, file_id):\n",
        "        \"\"\"Load ground truth blendshapes\"\"\"\n",
        "        path = os.path.join(self.base_path, 'spk08_ser', f'08_ser_spk08_{file_id}.npy')\n",
        "        return np.load(path).astype(np.float32)\n",
        "\n",
        "    def predict_single_file(self, file_id):\n",
        "        \"\"\"Predict blendshapes for single file\"\"\"\n",
        "        audio = os.path.join(self.base_path, 'ser/audio', f'spk08_{file_id}.wav')\n",
        "        phoneme = os.path.join(self.base_path, 'labels 08 srp', f'spk08_{file_id}.txt')\n",
        "\n",
        "        result = self.inference.predict(\n",
        "            audio,\n",
        "            phoneme_alignment_path=phoneme if os.path.exists(phoneme) else None\n",
        "        )\n",
        "        return result['blendshapes']\n",
        "\n",
        "    def compute_metrics(self, pred, gt):\n",
        "        \"\"\"Compute comprehensive metrics - UPDATED with Jaw and Velocity metrics\"\"\"\n",
        "        min_len = min(len(pred), len(gt))\n",
        "        pred, gt = pred[:min_len], gt[:min_len]\n",
        "\n",
        "        # 1. Basic metrics (Overall)\n",
        "        mse = np.mean((pred - gt) ** 2)\n",
        "        mae = np.mean(np.abs(pred - gt))\n",
        "        overall_corr, _ = pearsonr(pred.flatten(), gt.flatten())\n",
        "\n",
        "        # 2. Per-blendshape correlation (Mean of all 52)\n",
        "        per_bs_corr = []\n",
        "        for i in range(52):\n",
        "            if np.std(gt[:, i]) > 1e-6 and np.std(pred[:, i]) > 1e-6:\n",
        "                try:\n",
        "                    c, _ = pearsonr(pred[:, i], gt[:, i])\n",
        "                    if not np.isnan(c): per_bs_corr.append(c)\n",
        "                except: pass\n",
        "\n",
        "        # 3. Mouth-specific (28-51)\n",
        "        mouth_indices = list(range(28, 52))\n",
        "        mouth_mse = np.mean((pred[:, mouth_indices] - gt[:, mouth_indices]) ** 2)\n",
        "        mouth_mae = np.mean(np.abs(pred[:, mouth_indices] - gt[:, mouth_indices]))\n",
        "        mouth_corr, _ = pearsonr(pred[:, mouth_indices].flatten(), gt[:, mouth_indices].flatten())\n",
        "\n",
        "        # 4. Jaw-specific (24 - jawOpen) - KRITIƒåNO ZA LIP-SYNC\n",
        "        jaw_idx = 24\n",
        "        jaw_mae = np.mean(np.abs(pred[:, jaw_idx] - gt[:, jaw_idx]))\n",
        "        jaw_corr = 0\n",
        "        if np.std(gt[:, jaw_idx]) > 1e-6 and np.std(pred[:, jaw_idx]) > 1e-6:\n",
        "            jaw_corr, _ = pearsonr(pred[:, jaw_idx], gt[:, jaw_idx])\n",
        "\n",
        "        # 5. Dynamics & Velocity\n",
        "        pred_vel = np.diff(pred, axis=0)\n",
        "        gt_vel = np.diff(gt, axis=0)\n",
        "        velocity_mse = np.mean((pred_vel - gt_vel) ** 2)\n",
        "\n",
        "        # Velocity Correlation (da li se usta otvaraju istom brzinom)\n",
        "        vel_corr, _ = pearsonr(pred_vel.flatten(), gt_vel.flatten())\n",
        "\n",
        "        # Jitter / Smoothness\n",
        "        pred_jitter = np.mean(np.abs(np.diff(pred_vel, axis=0)))\n",
        "        gt_jitter = np.mean(np.abs(np.diff(gt_vel, axis=0)))\n",
        "\n",
        "        return {\n",
        "            'mse': mse,\n",
        "            'mae': mae,\n",
        "            'correlation': overall_corr,\n",
        "            'per_bs_corr_mean': np.mean(per_bs_corr) if per_bs_corr else 0,\n",
        "            'mouth_mse': mouth_mse,\n",
        "            'mouth_mae': mouth_mae,      # DODATO\n",
        "            'mouth_correlation': mouth_corr,\n",
        "            'jaw_mae': jaw_mae,          # DODATO\n",
        "            'jaw_correlation': jaw_corr, # DODATO\n",
        "            'velocity_mse': velocity_mse,\n",
        "            'velocity_correlation': vel_corr, # DODATO\n",
        "            'smoothness_ratio': pred_jitter / (gt_jitter + 1e-8),\n",
        "            'pred_jitter': pred_jitter,\n",
        "            'gt_jitter': gt_jitter\n",
        "        }\n",
        "\n",
        "    '''\n",
        "    def compute_metrics(self, pred, gt):\n",
        "        \"\"\"Compute comprehensive metrics\"\"\"\n",
        "        min_len = min(len(pred), len(gt))\n",
        "        pred, gt = pred[:min_len], gt[:min_len]\n",
        "\n",
        "        # Basic metrics\n",
        "        mse = np.mean((pred - gt) ** 2)\n",
        "        mae = np.mean(np.abs(pred - gt))\n",
        "\n",
        "        # Correlation (overall)\n",
        "        overall_corr, _ = pearsonr(pred.flatten(), gt.flatten())\n",
        "\n",
        "        # Per-blendshape correlation\n",
        "        per_bs_corr = []\n",
        "        for i in range(52):\n",
        "            if np.std(gt[:, i]) > 1e-6 and np.std(pred[:, i]) > 1e-6:\n",
        "                try:\n",
        "                    c, _ = pearsonr(pred[:, i], gt[:, i])\n",
        "                    if not np.isnan(c):\n",
        "                        per_bs_corr.append(c)\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "        # Velocity metrics\n",
        "        pred_vel = np.diff(pred, axis=0)\n",
        "        gt_vel = np.diff(gt, axis=0)\n",
        "        velocity_mse = np.mean((pred_vel - gt_vel) ** 2)\n",
        "\n",
        "        # Jitter\n",
        "        pred_jitter = np.mean(np.abs(np.diff(pred_vel, axis=0)))\n",
        "        gt_jitter = np.mean(np.abs(np.diff(gt_vel, axis=0)))\n",
        "\n",
        "        # Mouth-specific metrics\n",
        "        mouth_indices = list(range(28, 52))\n",
        "        mouth_mse = np.mean((pred[:, mouth_indices] - gt[:, mouth_indices]) ** 2)\n",
        "        mouth_corr, _ = pearsonr(pred[:, mouth_indices].flatten(), gt[:, mouth_indices].flatten())\n",
        "\n",
        "        return {\n",
        "            'mse': mse,\n",
        "            'mae': mae,\n",
        "            'correlation': overall_corr,\n",
        "            'per_bs_corr_mean': np.mean(per_bs_corr) if per_bs_corr else 0,\n",
        "            'velocity_mse': velocity_mse,\n",
        "            'smoothness_ratio': pred_jitter / (gt_jitter + 1e-8),\n",
        "            'pred_jitter': pred_jitter,\n",
        "            'gt_jitter': gt_jitter,\n",
        "            'mouth_mse': mouth_mse,\n",
        "            'mouth_correlation': mouth_corr\n",
        "        }\n",
        "    '''\n",
        "\n",
        "    def visualize_prediction(self, file_id, save_path='validation_plots'):\n",
        "        \"\"\"Create detailed visualization - UPDATED with specific Jaw and Mouth metrics\"\"\"\n",
        "        os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "        print(f\"üìä Visualizing: {file_id}\")\n",
        "\n",
        "        # Predict\n",
        "        pred = self.predict_single_file(file_id)\n",
        "        gt = self.load_ground_truth(file_id)\n",
        "\n",
        "        # Align\n",
        "        min_len = min(len(pred), len(gt))\n",
        "        pred = pred[:min_len]\n",
        "        gt = gt[:min_len]\n",
        "\n",
        "        # Compute metrics\n",
        "        metrics = self.compute_metrics(pred, gt)\n",
        "\n",
        "        # Create figure\n",
        "        fig = plt.figure(figsize=(22, 15))\n",
        "\n",
        "        # 1. Key mouth movements (jawOpen je ovde najbitniji)\n",
        "        ax1 = plt.subplot(3, 3, 1)\n",
        "        mouth_indices = [24, 28, 36, 37, 38]\n",
        "        mouth_names = ['jawOpen', 'mouthFunnel', 'mouthClose', 'mouthSmileL', 'mouthSmileR']\n",
        "\n",
        "        for idx, name in zip(mouth_indices, mouth_names):\n",
        "            ax1.plot(gt[:, idx], label=f'{name} (GT)', linestyle='--', alpha=0.7)\n",
        "            ax1.plot(pred[:, idx], label=f'{name} (Pred)', linewidth=2)\n",
        "\n",
        "        ax1.set_xlabel('Frame')\n",
        "        ax1.set_ylabel('Value')\n",
        "        ax1.set_title(f'Key Movements (Jaw Corr: {metrics[\"jaw_correlation\"]:.3f})')\n",
        "        ax1.legend(fontsize=7, ncol=2)\n",
        "        ax1.grid(alpha=0.3)\n",
        "\n",
        "        # 2. Error heatmap\n",
        "        ax2 = plt.subplot(3, 3, 2)\n",
        "        error = np.abs(pred - gt)\n",
        "        im = ax2.imshow(error.T, aspect='auto', cmap='hot', interpolation='nearest')\n",
        "        ax2.set_xlabel('Frame')\n",
        "        ax2.set_ylabel('Blendshape Index')\n",
        "        ax2.set_title('Absolute Error Heatmap')\n",
        "        plt.colorbar(im, ax=ax2, label='Error')\n",
        "\n",
        "        # 3. Per-blendshape correlation\n",
        "        ax3 = plt.subplot(3, 3, 3)\n",
        "        correlations = []\n",
        "        for i in range(52):\n",
        "            if np.std(gt[:, i]) > 1e-6 and np.std(pred[:, i]) > 1e-6:\n",
        "                try:\n",
        "                    c, _ = pearsonr(pred[:, i], gt[:, i])\n",
        "                    correlations.append(c if not np.isnan(c) else 0)\n",
        "                except: correlations.append(0)\n",
        "            else: correlations.append(0)\n",
        "\n",
        "        colors = ['green' if c > 0.8 else 'orange' if c > 0.6 else 'red' for c in correlations]\n",
        "        # MARKER ZA VILICU (index 24) - ƒçinimo ga prepoznatljivim\n",
        "        colors[24] = 'blue'\n",
        "        ax3.bar(range(52), correlations, color=colors, alpha=0.7)\n",
        "        ax3.axhline(y=0.7, color='blue', linestyle=':', label='Target Threshold')\n",
        "        ax3.set_xlabel('Index (Blue bar is Jaw)')\n",
        "        ax3.set_ylabel('Correlation')\n",
        "        ax3.set_title('Per-BS Correlation')\n",
        "        ax3.grid(alpha=0.3)\n",
        "\n",
        "        # 4. Velocity comparison\n",
        "        ax4 = plt.subplot(3, 3, 4)\n",
        "        pred_vel = np.linalg.norm(np.diff(pred, axis=0), axis=1)\n",
        "        gt_vel = np.linalg.norm(np.diff(gt, axis=0), axis=1)\n",
        "        ax4.plot(gt_vel, label='GT Vel', color='blue', alpha=0.5)\n",
        "        ax4.plot(pred_vel, label='Pred Vel', color='orange', alpha=0.8)\n",
        "        ax4.set_title(f'Motion Dynamics (Vel Corr: {metrics[\"velocity_correlation\"]:.3f})')\n",
        "        ax4.legend()\n",
        "        ax4.grid(alpha=0.3)\n",
        "\n",
        "        # 5. Distribution comparison\n",
        "        ax5 = plt.subplot(3, 3, 5)\n",
        "        ax5.hist(gt.flatten(), bins=50, alpha=0.4, label='GT', density=True, color='blue')\n",
        "        ax5.hist(pred.flatten(), bins=50, alpha=0.4, label='Pred', density=True, color='orange')\n",
        "        ax5.set_title('Value Distribution (Target: Overlap at 0)')\n",
        "        ax5.legend()\n",
        "\n",
        "        # 6. Mouth-only comparison\n",
        "        ax6 = plt.subplot(3, 3, 6)\n",
        "        mouth_indices = list(range(28, 52))\n",
        "        for i in range(0, 24, 6): # Prikazujemo svaku 6. rastegnutu usnu radi preglednosti\n",
        "            ax6.plot(gt[:, mouth_indices[i]], alpha=0.3, color='blue', linestyle='--')\n",
        "            ax6.plot(pred[:, mouth_indices[i]], alpha=0.8, color='orange')\n",
        "        ax6.set_title(f'Mouth Detail (MAE: {metrics[\"mouth_mae\"]:.4f})')\n",
        "        ax6.grid(alpha=0.3)\n",
        "\n",
        "        # 7-9. Metrics summary - NOVE METRIKE UKLJUƒåENE\n",
        "        ax7 = plt.subplot(3, 3, 7)\n",
        "        ax7.axis('off')\n",
        "\n",
        "        # Assessment logic based on LIPS (Mouth & Jaw)\n",
        "        lip_score = (metrics['mouth_correlation'] + metrics['jaw_correlation']) / 2\n",
        "        if lip_score > 0.80 and metrics['mouth_mae'] < 0.04:\n",
        "            assessment = 'üèÜ CHAMPIONSHIP LEVEL'\n",
        "        elif lip_score > 0.65 and metrics['mouth_mae'] < 0.06:\n",
        "            assessment = '‚úÖ EXCELLENT'\n",
        "        elif lip_score > 0.45:\n",
        "            assessment = 'üëç GOOD'\n",
        "        else:\n",
        "            assessment = '‚ö†Ô∏è NEEDS IMPROVEMENT'\n",
        "\n",
        "        metrics_text = f\"\"\"\n",
        "üìä OVERALL QUALITY\n",
        "{'='*35}\n",
        "MSE:              {metrics['mse']:.6f}\n",
        "MAE:              {metrics['mae']:.6f}\n",
        "Correlation:      {metrics['correlation']:.4f}\n",
        "\n",
        "üìà MOUTH & JAW (LIP-SYNC)\n",
        "{'='*35}\n",
        "Mouth Corr:       {metrics['mouth_correlation']:.4f}\n",
        "Mouth MAE:        {metrics['mouth_mae']:.4f}\n",
        "Jaw Corr:         {metrics['jaw_correlation']:.4f}\n",
        "Jaw MAE:          {metrics['jaw_mae']:.4f}\n",
        "\n",
        "üéØ DYNAMICS (ENERGY)\n",
        "{'='*35}\n",
        "Vel. Corr:        {metrics['velocity_correlation']:.4f}\n",
        "Smoothness Ratio: {metrics['smoothness_ratio']:.4f}\n",
        "\n",
        "‚≠ê STATUS: {assessment}\n",
        "        \"\"\"\n",
        "\n",
        "        ax7.text(0.05, 0.5, metrics_text, fontsize=11, family='monospace',\n",
        "                verticalalignment='center',\n",
        "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.2))\n",
        "\n",
        "        plt.suptitle(f'Championship Validation: {file_id}', fontsize=16, fontweight='bold')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        output_path = os.path.join(save_path, f'{file_id}_analysis.png')\n",
        "        plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "        print(f\"‚úÖ Saved: {output_path}\")\n",
        "        return metrics\n",
        "\n",
        "    '''\n",
        "    def visualize_prediction(self, file_id, save_path='validation_plots'):\n",
        "        \"\"\"Create detailed visualization\"\"\"\n",
        "        os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "        print(f\"üìä Visualizing: {file_id}\")\n",
        "\n",
        "        # Predict\n",
        "        pred = self.predict_single_file(file_id)\n",
        "        gt = self.load_ground_truth(file_id)\n",
        "\n",
        "        # Align\n",
        "        min_len = min(len(pred), len(gt))\n",
        "        pred = pred[:min_len]\n",
        "        gt = gt[:min_len]\n",
        "\n",
        "        # Compute metrics\n",
        "        metrics = self.compute_metrics(pred, gt)\n",
        "\n",
        "        # Create figure\n",
        "        fig = plt.figure(figsize=(20, 14))\n",
        "\n",
        "        # 1. Key mouth movements\n",
        "        ax1 = plt.subplot(3, 3, 1)\n",
        "        mouth_indices = [24, 28, 36, 37, 38]\n",
        "        mouth_names = ['jawOpen', 'mouthFunnel', 'mouthClose', 'mouthSmileL', 'mouthSmileR']\n",
        "\n",
        "        for idx, name in zip(mouth_indices, mouth_names):\n",
        "            ax1.plot(gt[:, idx], label=f'{name} (GT)', linestyle='--', alpha=0.7)\n",
        "            ax1.plot(pred[:, idx], label=f'{name} (Pred)', linewidth=2)\n",
        "\n",
        "        ax1.set_xlabel('Frame')\n",
        "        ax1.set_ylabel('Blendshape Value')\n",
        "        ax1.set_title('Key Mouth Movements')\n",
        "        ax1.legend(fontsize=7, ncol=2)\n",
        "        ax1.grid(alpha=0.3)\n",
        "\n",
        "        # 2. Error heatmap\n",
        "        ax2 = plt.subplot(3, 3, 2)\n",
        "        error = np.abs(pred - gt)\n",
        "        movement_mask = np.max(gt, axis=1) > 0.1\n",
        "\n",
        "        if movement_mask.sum() > 0:\n",
        "            error_subset = error[movement_mask][:200]\n",
        "            im = ax2.imshow(error_subset.T, aspect='auto', cmap='hot', interpolation='nearest')\n",
        "            ax2.set_xlabel('Frame')\n",
        "            ax2.set_ylabel('Blendshape Index')\n",
        "            ax2.set_title('Absolute Error Heatmap')\n",
        "            plt.colorbar(im, ax=ax2, label='Error')\n",
        "\n",
        "        # 3. Per-blendshape correlation\n",
        "        ax3 = plt.subplot(3, 3, 3)\n",
        "        correlations = []\n",
        "        for i in range(52):\n",
        "            if np.std(gt[:, i]) > 1e-6 and np.std(pred[:, i]) > 1e-6:\n",
        "                try:\n",
        "                    corr, _ = pearsonr(pred[:, i], gt[:, i])\n",
        "                    correlations.append(corr if not np.isnan(corr) else 0)\n",
        "                except:\n",
        "                    correlations.append(0)\n",
        "            else:\n",
        "                correlations.append(0)\n",
        "\n",
        "        colors = ['green' if c > 0.8 else 'orange' if c > 0.6 else 'red' for c in correlations]\n",
        "        ax3.bar(range(52), correlations, color=colors, alpha=0.7)\n",
        "        ax3.axhline(y=0.8, color='green', linestyle='--', linewidth=1.5, label='Excellent')\n",
        "        ax3.axhline(y=0.6, color='orange', linestyle='--', linewidth=1.5, label='Good')\n",
        "        ax3.set_xlabel('Blendshape Index')\n",
        "        ax3.set_ylabel('Correlation')\n",
        "        ax3.set_title('Per-Blendshape Correlation')\n",
        "        ax3.legend()\n",
        "        ax3.grid(alpha=0.3)\n",
        "\n",
        "        # 4. Velocity comparison\n",
        "        ax4 = plt.subplot(3, 3, 4)\n",
        "        pred_vel = np.linalg.norm(np.diff(pred, axis=0), axis=1)\n",
        "        gt_vel = np.linalg.norm(np.diff(gt, axis=0), axis=1)\n",
        "\n",
        "        ax4.plot(gt_vel, label='GT Velocity', color='blue', alpha=0.7, linewidth=1.5)\n",
        "        ax4.plot(pred_vel, label='Pred Velocity', color='orange', linewidth=2)\n",
        "        ax4.set_xlabel('Frame')\n",
        "        ax4.set_ylabel('Velocity Magnitude')\n",
        "        ax4.set_title('Motion Velocity')\n",
        "        ax4.legend()\n",
        "        ax4.grid(alpha=0.3)\n",
        "\n",
        "        # 5. Distribution comparison\n",
        "        ax5 = plt.subplot(3, 3, 5)\n",
        "        ax5.hist(gt.flatten(), bins=50, alpha=0.5, label='GT', density=True, color='blue')\n",
        "        ax5.hist(pred.flatten(), bins=50, alpha=0.5, label='Pred', density=True, color='orange')\n",
        "        ax5.set_xlabel('Blendshape Value')\n",
        "        ax5.set_ylabel('Density')\n",
        "        ax5.set_title('Value Distribution')\n",
        "        ax5.legend()\n",
        "        ax5.grid(alpha=0.3)\n",
        "\n",
        "        # 6. Mouth-only comparison\n",
        "        ax6 = plt.subplot(3, 3, 6)\n",
        "        mouth_indices = list(range(28, 52))\n",
        "        mouth_gt = gt[:, mouth_indices]\n",
        "        mouth_pred = pred[:, mouth_indices]\n",
        "\n",
        "        for i in range(0, 24, 4):\n",
        "            ax6.plot(mouth_gt[:, i], alpha=0.5, color='blue')\n",
        "            ax6.plot(mouth_pred[:, i], alpha=0.7, color='orange')\n",
        "\n",
        "        ax6.set_xlabel('Frame')\n",
        "        ax6.set_ylabel('Mouth Blendshapes')\n",
        "        ax6.set_title(f'Mouth Region (Corr: {metrics[\"mouth_correlation\"]:.3f})')\n",
        "        ax6.grid(alpha=0.3)\n",
        "\n",
        "        # 7-9. Metrics summary\n",
        "        ax7 = plt.subplot(3, 3, 7)\n",
        "        ax7.axis('off')\n",
        "\n",
        "        # Assessment\n",
        "        if metrics['correlation'] > 0.85 and metrics['mae'] < 0.05:\n",
        "            assessment = 'üèÜ CHAMPIONSHIP LEVEL'\n",
        "            color = 'green'\n",
        "        elif metrics['correlation'] > 0.75 and metrics['mae'] < 0.08:\n",
        "            assessment = '‚úÖ EXCELLENT'\n",
        "            color = 'darkgreen'\n",
        "        elif metrics['correlation'] > 0.60:\n",
        "            assessment = 'üëç GOOD'\n",
        "            color = 'orange'\n",
        "        else:\n",
        "            assessment = '‚ö†Ô∏è NEEDS IMPROVEMENT'\n",
        "            color = 'red'\n",
        "\n",
        "        metrics_text = f\"\"\"\n",
        "üìä QUALITY METRICS\n",
        "{'='*40}\n",
        "MSE:              {metrics['mse']:.6f}\n",
        "MAE:              {metrics['mae']:.6f}\n",
        "Correlation:      {metrics['correlation']:.6f}\n",
        "Per-BS Corr Avg:  {metrics['per_bs_corr_mean']:.6f}\n",
        "\n",
        "üìà MOUTH SPECIFIC\n",
        "{'='*40}\n",
        "Mouth MSE:        {metrics['mouth_mse']:.6f}\n",
        "Mouth Corr:       {metrics['mouth_correlation']:.6f}\n",
        "\n",
        "üéØ DYNAMICS\n",
        "{'='*40}\n",
        "Velocity MSE:     {metrics['velocity_mse']:.6f}\n",
        "Smoothness Ratio: {metrics['smoothness_ratio']:.6f}\n",
        "\n",
        "‚≠ê OVERALL\n",
        "{'='*40}\n",
        "{assessment}\n",
        "        \"\"\"\n",
        "\n",
        "        ax7.text(0.1, 0.5, metrics_text, fontsize=10, family='monospace',\n",
        "                verticalalignment='center',\n",
        "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))\n",
        "\n",
        "        plt.suptitle(f'Championship Validation: {file_id}', fontsize=16, fontweight='bold')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        output_path = os.path.join(save_path, f'{file_id}_analysis.png')\n",
        "        plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "        print(f\"‚úÖ Saved: {output_path}\")\n",
        "        return metrics\n",
        "    '''\n",
        "    def generate_championship_report(self, num_samples=10):\n",
        "        \"\"\"Generate comprehensive validation report\"\"\"\n",
        "        print(\"\\n\" + \"üèÜ\"*35)\n",
        "        print(\"CHAMPIONSHIP VALIDATION REPORT\")\n",
        "        print(\"üèÜ\"*35 + \"\\n\")\n",
        "\n",
        "        # Sample files\n",
        "        sample_files = random.sample(self.val_files, min(num_samples, len(self.val_files)))\n",
        "\n",
        "        all_metrics = []\n",
        "        for file_id in tqdm(sample_files, desc=\"Validating\"):\n",
        "            metrics = self.visualize_prediction(file_id)\n",
        "            metrics['file_id'] = file_id\n",
        "            all_metrics.append(metrics)\n",
        "\n",
        "        df = pd.DataFrame(all_metrics)\n",
        "\n",
        "        # Print results\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"üìä VALIDATION RESULTS\")\n",
        "        print(\"=\"*70)\n",
        "        print(df[['file_id', 'correlation', 'mae', 'mouth_correlation']].to_string(index=False))\n",
        "\n",
        "        # Summary statistics\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"üìà SUMMARY STATISTICS\")\n",
        "        print(\"=\"*70)\n",
        "        print(f\"Average Correlation: {df['correlation'].mean():.4f} (¬±{df['correlation'].std():.4f})\")\n",
        "        print(f\"Average MAE:         {df['mae'].mean():.6f} (¬±{df['mae'].std():.6f})\")\n",
        "        print(f\"Average Mouth Corr:  {df['mouth_correlation'].mean():.4f} (¬±{df['mouth_correlation'].std():.4f})\")\n",
        "\n",
        "        # Quality distribution\n",
        "        championship = (df['correlation'] > 0.85).sum()\n",
        "        excellent = ((df['correlation'] > 0.75) & (df['correlation'] <= 0.85)).sum()\n",
        "        good = ((df['correlation'] > 0.60) & (df['correlation'] <= 0.75)).sum()\n",
        "        needs_work = (df['correlation'] <= 0.60).sum()\n",
        "\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"üéØ QUALITY DISTRIBUTION\")\n",
        "        print(\"=\"*70)\n",
        "        print(f\"üèÜ Championship (>0.85):  {championship}/{len(df)} ({100*championship/len(df):.1f}%)\")\n",
        "        print(f\"‚úÖ Excellent (0.75-0.85): {excellent}/{len(df)} ({100*excellent/len(df):.1f}%)\")\n",
        "        print(f\"üëç Good (0.60-0.75):      {good}/{len(df)} ({100*good/len(df):.1f}%)\")\n",
        "        print(f\"‚ö†Ô∏è  Needs Work (<0.60):    {needs_work}/{len(df)} ({100*needs_work/len(df):.1f}%)\")\n",
        "\n",
        "        # Final assessment\n",
        "        avg_corr = df['correlation'].mean()\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"üéØ FINAL ASSESSMENT\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        if avg_corr > 0.85:\n",
        "            print(\"üèÜ MODEL STATUS: CHAMPIONSHIP LEVEL\")\n",
        "            print(\"   Outstanding performance! Ready for production!\")\n",
        "        elif avg_corr > 0.75:\n",
        "            print(\"‚úÖ MODEL STATUS: EXCELLENT\")\n",
        "            print(\"   Great performance! Suitable for most applications!\")\n",
        "        elif avg_corr > 0.60:\n",
        "            print(\"üëç MODEL STATUS: GOOD\")\n",
        "            print(\"   Solid performance, some room for improvement\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è MODEL STATUS: NEEDS IMPROVEMENT\")\n",
        "            print(\"   Consider additional training or tuning\")\n",
        "\n",
        "        # Save report\n",
        "        df.to_csv('championship_validation_metrics.csv', index=False)\n",
        "        print(f\"\\nüíæ Saved: championship_validation_metrics.csv\")\n",
        "        print(f\"üìÇ Visualizations in: validation_plots/\")\n",
        "\n",
        "        return df\n",
        "\n",
        "\n",
        "print(\"‚úÖ Championship Validator loaded!\")\n",
        "print(\"üìå Usage:\")\n",
        "print(\"   validator = ChampionshipValidator(\")\n",
        "print(\"       'checkpoints_championship/best_model.pt',\")\n",
        "print(\"       '/content/drive/MyDrive/AI-speak'\")\n",
        "print(\"   )\")\n",
        "print(\"   validator.generate_championship_report()\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Lf_qhNkbvP-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\"\"\"\n",
        "üèÜ POKRENI VALIDACIJU ODMAH\n",
        "Samo kopiraj i pokreni ovu ƒáeliju!\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================================\n",
        "# PODESI PUTANJE (promeni ako treba)\n",
        "# ============================================================================\n",
        "MODEL_PATH = 'checkpoints_championship/best_model.pt'  # Putanja do tvog modela\n",
        "BASE_PATH = '/content/drive/MyDrive/AI-speak'      # Putanja do podataka\n",
        "\n",
        "# ============================================================================\n",
        "# POKRENI VALIDACIJU\n",
        "# ============================================================================\n",
        "\n",
        "print(\"üîç Pokreƒáem validaciju...\\n\")\n",
        "\n",
        "# Kreiraj validator\n",
        "validator = ChampionshipValidator(\n",
        "    model_path=MODEL_PATH,\n",
        "    base_path=BASE_PATH,\n",
        "    device='cuda'\n",
        ")\n",
        "\n",
        "# Generi≈°i report (10 primera)\n",
        "validation_df = validator.generate_championship_report(num_samples=10)\n",
        "\n",
        "# ============================================================================\n",
        "# PRIKA≈ΩI GRAFIKE (ako si u Jupyter/Colab)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üìä PRIKAZ GRAFIKA\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import os\n",
        "\n",
        "# Prika≈æi prvih 3 grafika\n",
        "plot_dir = 'validation_plots/'\n",
        "if os.path.exists(plot_dir):\n",
        "    plots = sorted([f for f in os.listdir(plot_dir) if f.endswith('_analysis.png')])\n",
        "\n",
        "    print(f\"Pronaƒëeno {len(plots)} grafika. Prikazujem prva 3...\\n\")\n",
        "\n",
        "    for plot_file in plots[:3]:\n",
        "        img = mpimg.imread(os.path.join(plot_dir, plot_file))\n",
        "        plt.figure(figsize=(20, 15))\n",
        "        plt.imshow(img)\n",
        "        plt.axis('off')\n",
        "        plt.title(f'Validation: {plot_file}', fontsize=16)\n",
        "        plt.show()\n",
        "        print(f\"‚úÖ Prikazano: {plot_file}\\n\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Folder 'validation_plots/' ne postoji\")\n",
        "\n",
        "# ============================================================================\n",
        "# DODATNE STATISTIKE\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üìà DETALJNE STATISTIKE\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "print(\"Per-file breakdown:\")\n",
        "print(validation_df[['file_id', 'correlation', 'mae', 'mouth_correlation', 'smoothness_ratio']].to_string(index=False))\n",
        "\n",
        "print(f\"\\nüìä Najbolji fajl:\")\n",
        "best_idx = validation_df['correlation'].idxmax()\n",
        "print(f\"   File: {validation_df.loc[best_idx, 'file_id']}\")\n",
        "print(f\"   Correlation: {validation_df.loc[best_idx, 'correlation']:.4f}\")\n",
        "print(f\"   MAE: {validation_df.loc[best_idx, 'mae']:.6f}\")\n",
        "\n",
        "print(f\"\\nüìä Najgori fajl:\")\n",
        "worst_idx = validation_df['correlation'].idxmin()\n",
        "print(f\"   File: {validation_df.loc[worst_idx, 'file_id']}\")\n",
        "print(f\"   Correlation: {validation_df.loc[worst_idx, 'correlation']:.4f}\")\n",
        "print(f\"   MAE: {validation_df.loc[worst_idx, 'mae']:.6f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚úÖ VALIDACIJA ZAVR≈†ENA!\")\n",
        "print(\"=\"*70)\n",
        "print(f\"üìÇ CSV report: championship_validation_metrics.csv\")\n",
        "print(f\"üìÇ Grafici: validation_plots/\")"
      ],
      "metadata": {
        "id": "2YQOJUJpvpN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "import shutil\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# ==========================================\n",
        "# KONFIGURACIJA ZA ƒåUVANJE\n",
        "# ==========================================\n",
        "# Gde trainer ƒçuva modele (proveri da li se poklapa sa tvojim 'save_dir')\n",
        "LOCAL_CHECKPOINT_PATH = 'checkpoints_championship/best_model.pt'\n",
        "\n",
        "# Gde na Drive-u ≈æeli≈° da saƒçuva≈°\n",
        "DRIVE_FOLDER = '/content/drive/MyDrive/AI-speak/Final_Models'\n",
        "\n",
        "# Kreiraj folder na Drive-u ako ne postoji\n",
        "if not os.path.exists(DRIVE_FOLDER):\n",
        "    os.makedirs(DRIVE_FOLDER)\n",
        "    print(f\"‚úÖ Kreiran folder na Drive-u: {DRIVE_FOLDER}\")\n",
        "\n",
        "# Kreiranje imena fajla sa datumom i vremenom\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
        "destination_path = os.path.join(DRIVE_FOLDER, f'championship_model_best_{timestamp}.pt')\n",
        "\n",
        "# ==========================================\n",
        "# KOPIRANJE\n",
        "# ==========================================\n",
        "if os.path.exists(LOCAL_CHECKPOINT_PATH):\n",
        "    try:\n",
        "        shutil.copy(LOCAL_CHECKPOINT_PATH, destination_path)\n",
        "        print(\"=\"*50)\n",
        "        print(f\"üöÄ USPE≈†NO SAƒåUVANO NA DRIVE!\")\n",
        "        print(f\"üìç Lokacija: {destination_path}\")\n",
        "        print(\"=\"*50)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Gre≈°ka pri kopiranju: {e}\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è Fajl {LOCAL_CHECKPOINT_PATH} nije pronaƒëen. Proveri putanju treninga.\")\n",
        "\n",
        "# Opciono: Kopiraj i poslednji CSV izve≈°taj o metrici ako postoji\n",
        "if os.path.exists('championship_validation_metrics.csv'):\n",
        "    shutil.copy('championship_validation_metrics.csv',\n",
        "                os.path.join(DRIVE_FOLDER, f'metrics_{timestamp}.csv'))\n",
        "    print(f\"üìä Saƒçuvan i CSV izve≈°taj.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "ubsE6Vqrvp_V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}